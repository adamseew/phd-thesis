
%%%%%%%%%%%%%%%%%%%
%                 %
% Energy Models   %
%                 %
\chapter{Energy Models}
\label{cp:model}

\begin{chapquote}{\cite{ondruska2015scheduled}}
  ``Robots require energy to operate. Yet they only have access to limited energy storage during missions.''
\end{chapquote}

\vspace*{1em}

\lettrine{E}{nergy is an essential aspect} of many autonomous mobile robotics scenarios~\citep{mei2005case} and often a limiting factor to improving computing performance~\citep{horowitz2014computing}. In the previous chapters, we emphasized the growing importance of both computations and motion energy components. Indeed, limited energy availability against high computing performance requirements of autonomous aerial robots is the motivation for the energy-aware coverage planning and scheduling for autonomous aerial robots in this work. For this purpose, we first need an accurate energy model that predicts future energy consumption. We derive such a model in this chapter, providing different energy models predicting the energy of the motion and computations and the battery state of an aerial robot. For the former two, we first derive a way to accurately predict the energy spent running a given set of computations on the computing hardware. We then merge the resulting computations energy model with a motion energy model using some properties of our autonomous scenario.  The energy output of the motion model is the input of the battery model, which predicts the battery evolution over time.

We saw in \fref{cp:model}{Chapter} some computations energy and battery models, and we discussed some energy-aware approaches for aerial robots (and mobile robots broadly) performing coverage path planning (\Gls{acr:cpp}) or motion planning generally. In this chapter, we then use the previous literature and derive the energy models for our scenario: an autonomous aerial robot employed in coverage planning, monitoring an agricultural field. Although applied to a given use case, the approach is general in terms of the proposed methodology. In \fref{sec:comp-ener-model}{Section}, we derive the model for the energy of computations based on regressional modeling. In \fref{sec:battery-model}{Section}, we provide a battery model based on an equivalent electrical circuit, and in \fref{sec:periodic-model}{Section}, we derive a model for the motion that incorporates the computations energy model. We then use the models in \fref{cp:dyn}{Chapter} to plan and schedule altogether.

This chapter connects to the remainder of this work as follows. We provided some energy implications of autonomous aerial robots in \fref{cp:intro}{Chapter} and formulated the basic constructs in \fref{cp:pb}{Chapter}, including the concepts of computations and motion energy. We discussed the past energy modeling and efficiency studies in \fref{cp:soa}{Chapter}, along with other literature. We then use all the information we discussed in the previous chapters for the energy models in this chapter. In \fref{cp:dyn}{Chapter}, we use the models to replan the coverage and schedule the computations in an energy-aware fashion. The replanning uses some optimal control techniques that we introduce in \fref{cp:opt}{Chapter}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Energy Model of the Computations}
\label{sec:comp-ener-model}

This section describes the computations energy model to predict the energy consumption of a given configuration of computations. In \fref{sec:definitions}{Section} and the precision agriculture example in \fref{sec:computation-wise}{Section}, we parametrized the computations by a set of $\sigma$ computations parameters $c_{i}^\sigma:=\{c_{i,\rho+1},\dots,c_{i,\rho+\sigma}\}$, where $\rho$ is the number of path parameters. We parametrize the computations that impact the overall energy consumption (as we described in \fref{def:comps}{Definition}). For instance, in the agricultural scenario, we parametrize the computation object detection. We mean by parametrization that we enable the computations to be dynamically replanned with an appropriate choice of the parameters, and run on, e.g., a lower/higher rate requiring lower/higher power. Practically, this means that if our system is composed of $\sigma$ computations, the configuration of computations parameters $c_i^\sigma(\mathcal{T})$ for each stage $i$ over time $\mathcal{T}:=[t_0,t_l]$ (where $t_0,t_l$ are respectively the first time instant and the time instant when the aerial robot reaches the final point $\mathbf{p}_{\Gamma_l}$ in \fref{def:trigs}{Definition}) is a schedule of the computations. In \fref{cp:dyn}{Chapter}, we will derive an energy-aware schedule via the model that we propose in this section.
In the remainder of this section, we derive an energy model that maps any choice of parameters $c_{i}^{\sigma}$ to the power at any $t\in\mathcal{T}$, extending the past work on energy modeling for heterogeneous computing hardware.

In \fref{sec:model-hete-elem}{Section}, we summarize the heterogeneous elements modeling from \fref{sec:soa-ene-hete}{Section} and outline our approach, which consists of two layers. We detail the layers in \fref{sec:measurement-layer}{Sections}\fref{sec:predictive-layer}{--\hspace{-.8ex}} and describe an automated modeling tool we developed in \fref{sec:powprof}{Section} along with its configuration specification in \fref{sec:conf-spec}{Section}. The tool works well for computing hardware equipped with internal power meters, yet, some computing hardware does not provide any. We discuss how to model such hardware also in \fref{sec:powprof}{Section}.


\subsection{Model for the heterogeneous elements}
\label{sec:model-hete-elem}

We saw in \fref{cp:pb}{Chapter} that traditionally, models for computing hardware focus on a specific computing element, such as CPU or GPU, or model the elements heterogeneously. The models provide a regression function (measuring the power consumption over time), analytical (using some architectural parameters), or other expressions to infer future energy consumption. The regression functions, analytical or other expressions might depend on low-level architectural parameters and be employed in an energy-efficient selection of such parameters, including voltage and frequency. Alternatively, they might depend on high-level parameters (such as the computations parameters $c_{i}^\sigma$ in this work) and be employed in an energy-aware configuration of software (and hardware), e.g., the tasks allocation on the CPU cores. An expression depending on a configuration is more common for heterogeneous models, as we summarized in \fref{tab:energy-models}{Table}. We focus on these models; in \fref{cp:intro}{Chapter} and formally in \fref{def:comps}{Definition}, we assumed the aerial robot caries heterogeneous computing hardware for energy-demanding computations. We refer to \fref{sec:comp-ener-model}{Section} for an extensive discussion of some energy modeling approaches in the literature for CPUs and GPUs powered and heterogeneous computing hardware. 

\begin{figure}[h!]
  \centering 
  \includegraphics[width=.7\textwidth]{pictures/_DSC6224}
  \caption[NVIDIA Jetson Nano heterogeneous computing hardware]{NVIDIA Jetson Nano heterogeneous computing hardware that we model the computations energy on. The hardware in the image includes the Jetson Developer Kit board that the Nano heterogeneous board is mounted on and has a total size of 100x80 millimeters and a weight of approx. 140 grams.}  
  \label{fig:nano}\findex{NVIDIA Jetson!Nano}
\end{figure}
\begin{figure}[h!]   
  \centering 
  \includegraphics[width=.7\textwidth]{pictures/_DSC6220}
  \caption[NVIDIA Jetson TX2 heterogeneous computing hardware]{NVIDIA Jetson TX2 heterogeneous computing hardware mounted on a Jetson Developer Kit\findex{NVIDIA Jetson!Developer Kit} board as Nano in \fref{fig:nano}{Figure} with a total size of 170x170 millimeters and a weight of approx. 512 grams.}   
  \label{fig:tx2}\findex{NVIDIA Jetson!TX2}
\end{figure}
\begin{figure}[h!]
  \centering
  \includegraphics[width=.7\textwidth]{pictures/_DSC6222}
  \caption[ODROID XU3 heterogeneous computing hardware]{ODROID XU3 heterogeneous computing hardware with a total size of 94x70 millimeters and a weight of 70 grams.}
  \label{fig:odroid}\findex{ODROID XU3}
\end{figure}

Our approach is based on our work~\citep{seewald2019coarse} on heterogeneous computing devices' energy modeling and generic, modeling virtually a wide range of heterogeneous computing hardware energy consumption in real use-cases with little users effort. We use statistical methods to derive a regression-based model segmented into two layers. In the first, the measurement layer\findex{measurement layer}, we map the time to the power, and in the second, the predictive layer\findex{predictive layer}, we map the computations configuration $c_{i}^\sigma(t)$ to the power. To generate a model, our approach inputs a user-defined configuration file, which simply specifies the computations along with the computation parameter constraint sets $\mathcal{S}_{i,k}$ in \fref{def:stage}{Definition}, per each computation $k\in[\sigma]_{>0}$ and stage $i\in[l]_{>0}$ (or $[n]_{>0}$ if the computations are specified along the primitive paths and reiterated when the aerial robot reaches $\mathbf{p}_{\Gamma_n}$ with a shift $\mathbf{d}$ in \fref{fig:state-machine}{Figure}). From the configuration, an automated modeling tool termed \powprof{} measures the energy consumption of a discrete set of configurations $c_{i,\rho+j}\in\mathcal{S}_{i,j}$, per each computation parameter $j\in[\sigma]_{>0}$ and derives the measurement layer. The tool allows merging then a set of measurement layers into a predictive layer via linear regression.
As heterogeneous computing hardware, we use different devices, such as NVIDIA Jetson Nano\findex{NVIDIA Jetson!Nano}, TX2\findex{NVIDIA Jetson!TX2}, and TK1\findex{NVIDIA Jetson!TK1} in \fref{fig:nano}{Figures}\fref{fig:tx2}{--\hspace*{-.8ex}} and \fref{fig:tk1}{} and ODROID XU3\findex{ODROID XU3} in \fref{fig:odroid}{Figure}. 
\begin{table}[h!]
  \footnotesize\fontfamily{phv}\selectfont
    \begin{tabularx}{\textwidth}{|X|*{4}{l|}}\hline
      Hardware & CPU & GPU & Memory & Sensors \\
      \hline
      NVIDIA Jetson Nano & -A57 & NVIDIA Maxwell & 4 GB LPDDR4 RAM & \cmark\\
      NVIDIA Jetson TX2 & -A57 & NVIDIA Pascal & 8 GB LPDDR4 RAM, 32 GB NV & \cmark\\
      NVIDIA Jetson TK1 & -A15 & NVIDIA Kepler & 1 GB DDR3L RAM, 16 GB NV & \xmark\\
      ODROID XU3 & -A15, -A7 & MALI & 2 GB LPDDR3 RAM & \cmark
      \\\hline
    \end{tabularx}
    \caption[Mobile computing hardware explicitly analyzed in this work]{Mobile computing hardware explicitly analyzed in this work. All the CPUs are ARM Cortex. The majority of the embedded boards provide random access memory (RAM)\findex{memory!random access}, and some a non-volatile (NV) memory\findex{memory!non-volatile}. All the boards have energy measuring capabilities except NVIDIA Jetson TK1.}
    \label{tab:hws}
\end{table}
We summarize the computing hardware that we explicitly consider in this work in \fref{tab:hws}{Table}. These devices are commonly employed in robotics literature to power complex computations. For instance, Jetson TX2 has been employed in path planning\findex{path planning}~\citep{dharmadhikari2020motion,ryou2018applying}, simultaneous localization and mapping\findex{simultaneous localization and mapping} (\Gls{acr:slam})~\citep{aldegheri2019data}, and object detection\findex{object detection} via convolutional neural networks\findex{convolutional neural network} (\Gls{acr:cnn}s)~\citep{william2019aerial}. Jetson Nano in SLAM and CNNs~\citep{peng2019evaluating,wang2020yolo,alexey2021autonomous}, and ODROID XU3~\citep{bhat2019power,papachristos2015aerial,giusti2016machine} and Jetson TK1~\citep{gong2016low,holper2017cyber} in similar applications. Although we report some in this paragraph, heterogeneous embedded boards power computations in many other mobile robots use-cases, thus possibly broadening the applicability of our work, which we further in \fref{cp:conc}{Chapter}. In the remainder of this section, we detail our energy model for the heterogeneous computing hardware.

\subsection{Measurement layer}\findex{measurement layer}
\label{sec:measurement-layer}

The measurement layer is the basic building block of the computations energy model: one or more measurement layers form the predictive layer--the model output--which we describe in \fref{sec:predictive-layer}{Section}. For a specific computations parameters configuration $c_i^\sigma(t)$, the measurement layer maps the time $t\in[t_0,t_f]:=\mathcal{T}\subset\mathbb{R}_{>0}$ (the final and initial time instants $t_0,t_f$ are given) to the power measured in watts\findex{watts}, the energy measured in joules\findex{joules} (watts per unit of time), and the battery state of charge (\Gls{acr:soc})\findex{state of charge} expressed in percentages. The measurement layer thus provides a primitive model for $c_i^\sigma$ of power, energy, and SoC over a given time interval. The derivation of the layer is automated with \powprof{}, the modeling tool that we describe in detail in \fref{sec:powprof}{Section}, which outputs the layer after executing $c_i^\sigma$ on $\mathcal{T}$. Additionally, the model (and the \powprof{} tool) can output the triplet of metrics from the measurement layer per each energy sensor: some computing hardware that we analyze indeed provide different sensors for different computing elements, i.e., NVIDIA Jetson TX2, Nano, and ODROID XU3 boards all provide energy-sensing capabilities for CPU, GPU, overall, and/or memory.

Let us define the measurement layer formally, assuming there are one or more energy sensors or other energy measuring devices (these include, e.g., internal power resistors or shunt resistors, amperometers, and multimeters) in \fref{def:measur-layer}{Definition}.

\begin{highlight}
  \begin{defn}[Measurement layer]\label{def:measur-layer}
    Given a specific energy measuring device, computations parameters configuration $c_i^\sigma(t)$, and an initial and final time instant $t_0,t_f$ such that $t\in\mathcal{T}:=[t_0,t_f]$, the \textit{measurement layer} is the function
    $\mathbf{g}:\mathbb{Z}_{>0}\times\mathbb{Z}^\sigma\times\mathcal{T}\rightarrow\mathbb{R}^3$.
    It returns the power in watts, energy in joules, and SoC in percentages of an energy measuring device, a configuration of computations parameters, and time interval.
  \end{defn}
\end{highlight}

The measurement layer physically samples the computing hardware for $\mathcal{T}$ and returns the metrics, but sampling-to-completion is also possible, where a configuration runs up until it terminates rather than for a given interval. In the latter eventuality, $\mathcal{T}$ is ${\emptyset}$.

As an instance of the measurement layer, let us return briefly to the precision agriculture example in \fref{sec:flight-plan}{Section}, which describes the agricultural use-case (the aerial robot detects ground hazards and communicates the detections to a ground station). The computations parameters in \fref{sec:computation-wise}{Section} are $c_{i,2}$, the frames per second (\Gls{acr:fps})\findex{frames per second} rate, and $c_{i,3}$, encryption or no encryption of the robot---ground station data link. The configurations $c_{i,2}(t)\in\mathcal{S}_{i,2}$, $c_{i,3}(t)\in\mathcal{S}_{i,3}$ have any value within the constraint sets in \frefeqM{eq:encr-comp-const}{eq:cnn-comp-const}. The constraint sets are the same in each stage  $i$ in the plan $\Gamma$ except for the circles where the aerial robot travels the turns out of the boundaries, and the detections are inhibited. To build the measurement layer, we can discretize the configurations with a given $\delta_1,\delta_2$ for parameters $c_{i,2}$ and $c_{i,3}$. The measurement layer is then built by sampling the power for $\mathcal{T}$, one for all the possible configurations
\begin{equation}\label{eq:meas-layer-lin-sampl}
  c_i^\sigma:=\{c_{i,2},c_{i,3}\mid\forall j,k\in\mathbb{Z},\, \underline{c}_{i,2}+j\delta_1\in\mathcal{S}_{i,2},\underline{c}_{i,3}+k\delta_2\in\mathcal{S}_{i,3}\}.
\end{equation}

A value can be, e.g, $\delta_1=\delta_2=2$, so that there are ten configurations and consequently ten measurement layers. The \powprof{} tool automatically builds the layers, storing the results in comma-separated values (\Gls{acr:csv})\findex{comma-separated values} files (we see further the tool in \fref{sec:powprof}{Section}).
\frefeq{eq:meas-layer-lin-sampl} provides a way to sample the search space linearly, but other sampling strategies are equally possible, such as exponential sampling
\begin{equation}\label{eq:meas-layer-exp-sampl}
  c_{i}^\sigma:=\{c_{i,2},c_{i,3}\mid\forall j,k\in\mathbb{Z},\, {\delta_1}^j\in\mathcal{S}_{i,2},{\delta_2}^k\in\mathcal{S}_{i,3}\},
\end{equation}
where $\delta_1,\delta_2$ are now bases, or random sampling with the condition merely $c_{i,j}\in\mathbb{Z}_{>0}$. Currently, the \powprof{} tool supports automated linear and exponential sampling and complex sampling formed by different sampling strategies~\citep{seewald2019coarse}, e.g.,
\begin{equation}
  c_{i}^\sigma:=\{\{c_{i,2}\mid\forall k\in\mathbb{Z},\, \underline{c}_{i,2}+k\delta_1\in\mathcal{S}_{i,2}\},\{c_{i,3}\mid\forall k\in\mathbb{Z},{\delta_2}^k\in\mathcal{S}_{i,3}\}\}.
\end{equation}
Parameter ranges ($\mathcal{S}_{i,2},\mathcal{S}_{i,3}$) choice is dictated by the range at which the computations run at runtime, whereas $\delta$s choice is made so that the modeling terminates in a reasonable amount of time~\citep{seewald2019coarse}.

With the measurement layer described in this section, we know the power and other energy metrics of the (measured) configurations. However, we want to predict the energy consumption for any configuration of parameters in the constraint sets (and not only the sampled ones). We address this latter requirement in the next section, merging the measurement layers with linear regression.

\subsection{Predictive layer}\findex{predictive layer}
\label{sec:predictive-layer}

The predictive layer describes coarse-grained metrics, such as the power over FPS rate, mapping them to the metrics from the measurement layer, thus providing energy data for each configuration of parameters (or for each scheduling policy). To this end, it uses the set of measurement layers, building a two-by-two linear regression between consecutive layers. In the precision agriculture example with ten measurement layers, the predictive layer consists of regression between data points $\{c_{i,2},c_{i,3}\}$ for all the possible computations parameters (recall that a measurement later corresponds to a sampled computations configuration), opposed to the sampled ones in \fref{sec:measurement-layer}{Section}. \fref{def:comp-ener}{Definition} details the resulting model.

\begin{highlight}
  \begin{defn}[Predictive layer]\label{def:comp-ener}
    Given a specific energy measuring device and computations parameters configuration $c_i^\sigma(t)$ \textit{predictive layer} is the function $g:\mathbb{Z}_{\geq 0}\times\mathbb{Z}^\sigma\rightarrow\mathbb{R}^3$. It returns the power in watts, energy in joules, and SoC in percentages of any configuration of parameters within the constraint sets.
  \end{defn}
\end{highlight}

Analogously to the measurement layer, there can be various energy measuring devices, resulting in multiple triples of energy, power, and SoC, one per device. The predictive layer returns the same metrics of the measurement layer in \fref{def:measur-layer}{Definition}, without physically sampling the computing hardware but using the stored measurement layers. Indeed due to a potentially large search space in computational energy modeling, it is critical to infer the energy properties of the entire search space from a subset of all the possible samples~\citep{lee2006statistically,lee2006accurate,bailey2014adaptive}. The linear regression to infer such properties utilizes a method that we term the approximation method\findex{approximation method}~\citep{seewald2019coarse}. It builds a linear regression between two adjacent data points rather than merely for all the data points. Indeed we do not assume an apriori knowledge of the computations energy evolution with explicit models for all the data points such as linear or exponential, but rather model the energy linearly on tuples of data points~\citep{seewald2019coarse}.

Given an (unsampled) configuration $c_i^\sigma$, the predictive layer is approximated with
\begin{equation}
  g(c_i^\sigma)=(\mathbf{g}(\lceil c_i^\sigma\rceil,\mathcal{T}_1)-\mathbf{g}(\lfloor c_i^\sigma\rfloor),\mathcal{T}_2)(c_i^\sigma-\lfloor c_i^\sigma\rfloor)/(\lceil c_i^\sigma\rceil-\lfloor c_i^\sigma\rfloor)+\mathbf{g}(\lfloor c_i^\sigma\rfloor,\mathcal{T}_2),
\end{equation}
where $\lceil c_i^\sigma\rceil,\lfloor c_i^\sigma\rfloor$ are the two adjacent measurement layers of the computations configuration $c_i^\sigma$ (e.g., if we use $\delta_1=2$ in \frefeq{eq:meas-layer-lin-sampl}, $c_i^\sigma$ with just the parameter $c_{i,2}$ has $\lfloor c_i^\sigma\rfloor$ equal to two and $\lceil c_i^\sigma\rceil$ to four), and $\mathcal{T}_1,\mathcal{T}_2$ are the two time intervals in the measurement layer for configurations $\lfloor c_i^\sigma\rfloor,\lceil c_i^\sigma\rceil$ respectively.
We illustrate the principle in \fref{}{Figure}. % todo the figure

\subsection{The {\tt powprofiler} tool}\findex{powprofiler@\texttt{powprofiler}}
\label{sec:powprof}

The \powprof{}\footnote{The tool can be retrieved from \url{https://github.com/adamseew/powprofiler}} tool is an automated profiling and modeling utility that generates the measurement layers for a discrete set of possible computations configurations (automated profiling) and merges these layers later, providing the predictive layer (modeling). We proposed an early version of the tool earlier in our work~\citep{teamplayd43,seewald2019coarse}, which we extended later to support per-component energy modeling in a dataflow computational network~\citep{seewald2019component}, and integrated~\citep{zamanakos2020energy} with Robot Operating System (ROS) middleware~\citep{quigley2009ros}\findex{Robot Operating System}. The tool is written in C++\findex{C++} and distributed under an MIT license\findex{MIT license}. It supports all the computing hardware explicitly mentioned in this work, i.e., NVIDIA Jetson TX2, Nano, and TK1 and ODROID XU3 in \fref{tab:hws}{Table}. It is predisposed further for extensibility supporting possibly any Linux-based\findex{Linux} computing hardware that provides energy measuring devices or that alternatively provide a mechanism to measure the energy with an external device.

The tool uses an object-oriented programming\findex{object-oriented programming} approach~\citep{stroustrup1988what,wegner1990concepts}, where each computing hardware has its own class that inherits from the class {\small\tt sampler}\findex{class!sampler@\texttt{sampler}} the functions {\small\tt get\_sample}\findex{function!getsample@\texttt{get\_sample}} and {\small\tt dryrun}\findex{function!dryrun@\texttt{dryrun}}. The former function returns a power measurement from all the measuring devices on specific computing hardware (power for the measurement layer in \fref{sec:measurement-layer}{Section} for, e.g., CPU, GPU, overall, etc\dots), and the latter simply attempts to read from the measuring devices returning a boolean value indicating if the attempt was successful. The tool contains classes {\small\tt sampler\_tx2}\findex{class!samplertx2@\texttt{sampler\_tx2}}, {\small\tt sampler\_nano}\findex{class!samplernano@\texttt{sampler\_nano}}, and {\small\tt sampler\_odroid}\findex{class!samplerodroid@\texttt{sampler\_odroid}} already implementing the necessary utilities to store the models from the computing hardware that we explicitly analyze.

The function {\small\tt get\_sample} further relies on a specific data type, which we term {\small\tt vectorn}\findex{data type!vectorn@\texttt{vectorn}}. Each value in {\small\tt vectorn} has its flag, indicating the metric and the measuring device. For instance {\small\tt power\_cpu}, {\small\tt soc\_gpu} are two flags indicating that the metric is for the power and the measuring device is of
the CPU and the SoC of the GPU respectively. The enumeration {\small\tt vectorn\_flags} contains all the flags. The tool stores a set of {\small\tt vectorn}s sampled at discrete intervals (\powprof{} is highly personalizable, allowing to change the frequency via the configuration specification in \fref{sec:conf-spec}{Section}) in another structure termed {\small\tt pathn}\findex{data type!pathn@\texttt{pathn}}. Each {\small\tt pathn} corresponds to a measurement layer in \fref{sec:measurement-layer}{Section}. The tool further provides mechanisms, such as the overload of the constructor, to automatically load the layers from a previously stored CSV file. Internally the tool stores {\small\tt pathn}s (the measurement layers) in a wrapper, {\small\tt model\_1layer}\findex{class!model1layer@\texttt{model\_1layer}}, which contains information such as the parameters configuration and the set $\mathcal{T}$; {\small\tt model\_2layer}\findex{class!model2layer@\texttt{model\_2layer}} is then another internal structure that returns the predictive layer in \fref{sec:predictive-layer}{Section}. 
\begin{figure}[h!]
  \centering 
  \includegraphics[width=.7\textwidth]{pictures/_DSC6228}
  \caption[NVIDIA Jetson TK1 heterogeneous computing hardware]{NVIDIA Jetson TK1 heterogeneous computing hardware mounted on a Toradex Ixora\findex{Toradex Ixora} carrier board with a total size of 125x95 millimeters and a weight of approx. 160 grams.} 
  \label{fig:tk1}\findex{NVIDIA Jetson!TK1}
\end{figure}
In this setting, NVIDIA Jetson TK1 computing hardware does not include any energy measuring device. \powprof{} allows an external device in the sense that it can import data in the model directly through the overload of the constructor into a measurement layer (and therefore a set of measurement layers into a predictive layer). An early instance of our work~\citep{seewald2019hlpgpu} consisted of three hardware units for the purpose of this latter energy modeling of the TK1 computing hardware, similarly to another study in the literature~\citep{calore2015energy}. The main hardware unit in the early instance was the computing hardware itself, whereas the others were a multimeter and a workstation that interprets the data from the multimeter for subsequent processing by the tool~\citep{seewald2019coarse}.

The tool further interoperates with ROS middleware, generating a measurement layer for configurations of computations implemented in the middleware. It can be imported in an existing project as a library; in C/C++ by simply adding the preprocessor's directive\findex{preprocessor} {\small\tt \#include} with {\small\tt <powprof/async.h>}. In a setting where an energy-expensive ROS node is a computation (we implement both the CNN detection and encryption in \fref{sec:computation-wise}{Section} as ROS nodes), the user simply instances {\small\tt model\_1layer} with a specific computations configuration and calls function {\small\tt start}\findex{function!start@\texttt{start}} to start profiling, and {\small\tt stop}\findex{function!stop@\texttt{stop}} to stop. The latter then returns a measurement layer. The modeling can, in this fashion, happen online by running different computations configurations and generating an appropriate computations energy model corresponding to a realistic run-time computations load.

Alternatively to ROS middleware, the tool runs from a configuration specification, detailing each computation, the constraints sets, and the $\delta$s, along with some other, model-specific details. We discuss such configuration specification in the next section.

\subsection{Configuration specification}
\label{sec:conf-spec}

The configuration specification is simply a way to communicate the plan $\Gamma$ to the \powprof{} tool, along with some other model-specific details. To run \powprof{} with a configuration specification, the user invokes the command {\small\tt powprofile}, followed by the path of the configuration. If, for instance, the configuration specification is stored in {\small\tt config.cfg} in the current directory, the user invokes {\small\tt powprofile config.cfg}. The tool then parses the configuration specification to reconstruct the computations configurations and the constraints sets, to  automatically generate measurement layers and consequently provide a predictive layer. The configuration specification starts with the line {\small\tt [settings]} that indicates to \powprof{} all the following lines are a configuration specification. In the lines that follow, it contains a set of key-value properties delimited by an equal char. The property {\small\tt frequency}\findex{frequency} indicates the frequency measured hertz the tools samples at (e.g., with ten seconds, the property is set as {\small\tt frequency=10}). The property {\small\tt h} is the integration step\findex{integration step} the battery model\findex{battery model} integrates at (we discuss further the battery model in \fref{sec:battery-model}{Section}). The property {\small\tt directory} indicates the path where the models are stored. Additionally, the tool allows an arbitrary number of commands and white spaces, and the parser does not require a specific indentation. Any data followed by char {\small\tt \#} are ignored up to the next line, allowing to write eventual comments.

The following set of lines specifies the configuration $c_i^\sigma$ for each computation parameter $c_{i,\rho+1},c_{i,\rho+2},\dots,c_{i,\rho+\sigma}$, starting with the line {\small\tt [components]}, followed by {\small\tt [component.computation]} where {\small\tt computation} is a string uniquely identifying each computation (there cannot be two computations with the same string, but the name is arbitrary). Per each computation, the configuration file then contains a set of key-values properties. The property {\small\tt src} indicates the executable source of the computation. If \powprof{} runs with a configuration specification rather than a library, we assume the source accepts as arguments the configurations, e.g., $c_{i,\rho+1},\dots$ along with other eventual arguments. The following properties then specify these arguments, ordered as they appear in the configuration specification. The property {\small\tt range} indicates that the argument is a computation parameter. Let us assume the parameter is $c_{i,\rho+1}$. If it is sampled linearly as in \frefeq{eq:meas-layer-lin-sampl}, the value contains $\underline{c}_{i,\rho+1}$, $\overline{c}_{i,\rho+1}$, and $\delta$ delimited by commas, where $\delta$ is the step to sample $c_{i,\rho+1}$ in a reasonable amount of time in \frefeq{eq:meas-layer-lin-sampl}. If it is sampled exponentially as in \frefeq{eq:meas-layer-exp-sampl}, the value contains the same data as before, but for $\delta$ that is expressed {\small\tt pow(}$\delta${\small\tt )} and $\delta$ is the base.
Additional properties are then: {\small\tt fixed} that indicates another eventual argument the computation might have (that is not a computation parameter), and {\small\tt runtime} the value $t_f-t_0$. If {\small\tt runtime} is not specified, the tool assumes $\mathcal{T}=\emptyset$.

---










\section{\color{red}Battery Model}
\label{sec:battery-model}

%AdS: you are right but this is purely copy/paste from ijpp so the idea here is to have indeed a short text on battery model and then the 'Equations' in the 'Derivation of differential battery model'

\subsection{\color{orange}Batteries for aerial robots}

\subsection{\color{cyan}Derivation of differential battery model}

The battery model is a mathematical abstraction that models draining energy from a battery used to power the mobile robot. It was derived from the state-space problem representation of the empirical battery model through an equivalent electrical circuit, presented by Hasan et al.~\cite{hasan2018exogenous}. The battery evolution model can be represented in this way using an ordinary differential equation that is a function of the power drained from the system, and represents the SoC of the battery at a given instant
\begin{equation}\label{eq:battery-model-1}
  \frac{d}{dt}\mathrm{SoC}(t)=-\frac{I_\mathrm{int}(t)}{Q_c},
\end{equation}
\begin{equation}\label{eq:battery-model-2}
  I_\mathrm{int}(t)=
    \frac{U_\mathrm{int}-
    \sqrt{
      U_\mathrm{int}^2-
      4\cdot R_{\mathrm{int}}\cdot U_\mathrm{sta}\cdot I_{\mathrm{load}}(t)}
    }{2\cdot R_\mathrm{int}},
\end{equation}
where $Q_c$ is the constant nominal capacity, $U_\mathrm{int}$ is the internal battery voltage, $I_\mathrm{int}$ is the current load that depends on the power requirements, $R_\mathrm{int}$ is the internal resistance of the battery, $U_\mathrm{ext}$ is the external battery voltage, $U_{\mathrm{sta}}$ is the stabilized voltage (a fixed value determined by the load of the system), and $I_\mathrm{load}$ is the current required by the load. The constants are chosen to describe a small drone battery. The external battery voltage $U_{\mathrm{ext}}$ can be also expressed as follows:
\begin{equation}
U_{\mathrm{ext}}(t)=U_{\mathrm{int}}-R_{\mathrm{int}}\cdot I_{\mathrm{int}}(t)
\end{equation}

The approach allows modeling the computational system not only in terms of the overall power consumption but also in terms of the actual amount of power drained from the battery. This is especially important with a mobile robot dependent on a limited and time-dependent energy supply, running computationally heavy parallel algorithms. Critically, although this mathematical model is a simple abstraction, it models how a stable power consumption over time can induce less drain from the battery compared to using the same amount of energy distributed in a number of spikes. The battery model allows this information to be determined for specific usage scenarios and thus provides a useful way to determine what configuration corresponds to the best power usage combination for a mobile use case.


\section{\color{cyan}Energy Model of the Motion}

% This is just IRC paper one (the one with hector). The current model is the Periodic energy model

\subsection{\color{cyan}Mechanical energy of the aerial robot}

In this subsection, a regression technique to build a mechanical energy model is described. The model is specific for an Opterra drone, but an equivalent technique can be applied to any fixed-wing drone. Key to the technique is the distinction between three phases of the flight: take-off, cruise, and landing. The distinction between these three phases follows from experimental data, as we observed that different flights present similar behavior and almost identical tendencies. This means that each phase has a different altitude and overall time evolution concerning motor torque and power drain. In particular, less variability in energy evolution is observed during the cruise and take-off. This applies when we analyze a single test flight, and when we analyze a set of flights regarding their phases. In the latter, we observed little variability in the cruise phase of different flights, as this is performed mostly by the autopilot with variability often due to wind conditions, while take-off of a fixed-wing drone usually presents a similar set of controls with little variability. Landing is observed to be the phase that presents the largest variability in the collected data, as the procedure requires specific and conditions-dependent maneuvers. Furthermore, glide slope and ground effect, specific to landing site conditions, also affect the controls sequence.

A weighted average time derived from collected data of 28 seconds, 10 minutes, and 1 minute, for respectively take-off, cruise, and landing, is assumed to model a test mission that we use in this paper. The information is used in the regression technique to map time to the current energy consumption. Regression itself consists of a phase-specific third-order Fourier series, as the data often presents a periodic behavior and tends to diverge for higher orders while not mapping precisely to the power evolution in time for lower ones. The following equation represents the power as a function of time $t$ for each of the three phases:
\begin{equation}\label{eq:mechanical-model-1}
  f(t)=\sum_{n=0}^{3}{a_n\cos{\left(\frac{nt}{\xi}\right)} + b_n\sin{\left(\frac{nt}{\xi}\right)}},
\end{equation}
where $a_n$, $b_n$ are the Fourier series coefficients,  $\xi$ is a characteristic time, and:
\begin{equation}\label{eq:mechanical-model-2}
  \begin{split}
    f(t),t,a_n,b_n,\xi&\in\mathbb{R},
  \end{split}
\end{equation}
The Mechanical energy evolution in time can be expressed through a three-dimensional vector $\mathbf{f}$:
\begin{equation}\label{eq:mechanical-model-3}
  \mathbf{f}(t)=
  \begin{bmatrix}
    f_1(t)& f_2(t)& f_3(t)
  \end{bmatrix}^T,
\end{equation}
where $f_1(t)$, $f_2(t)$, $f_3(t)$ are the energy evolutions in time for take-off, cruise, and landing respectively.

The mechanical energy model from Equations~(\ref{eq:mechanical-model-1}--\ref{eq:mechanical-model-3}), can also be expressed in a matrix form:
\begin{equation}\label{eq:mechanical-model-4}
    \begin{split}
    \mathbf{f}(t)=&
    \begin{bmatrix}
      {}^1a_0 & {}^1b_0 & \cdots & {}^1a_3 & {}^1b_3 \\
      {}^2a_0 & {}^2b_0 & \cdots & {}^2a_3 & {}^2b_3 \\
      {}^3a_0 & {}^3b_0 & \cdots & {}^3a_3 & {}^3b_3
    \end{bmatrix}
    \begin{bmatrix}
      \cos\frac{0t}{\xi} \\ \sin\frac{0t}{\xi} \\ \vdots \\ \cos\frac{3t}{\xi} \\  \sin\frac{3t}{\xi}
    \end{bmatrix}.
  \end{split}
\end{equation}

The constants $a_n$, $b_n$, and $\xi$, for the regressions $f_i(t)$ with $i\in{1,2,3}$ (and thus for $\mathbf{f}(t)$) were found using the Levenberg-Marquardt algorithm implemented in Matlab. The above procedure, however, just accounts for one flight. A further analysis concerns the generation of power trajectories through the technique for each test flight. An interpolating curve is then built using a probabilistic approach, that simply indicates a weight for a given flight $k$. This can be particularly useful as some flights can be affected by other conditions such as wind, temperature, or battery state at the beginning of the mission, thus affecting the model behavior unexpectedly. Given $k$ flights and a vector of weights $\mathbf{w}$, the mechanical model can be in this fashion expressed using an equivalent expression to Equation~(\ref{eq:mechanical-model-2}):
\begin{equation}\label{eq:mechanical-model-5}
  \widetilde{\mathbf{f}}(t)=
  \begin{bmatrix}
    f_{1,1}(t)& f_{1,2}(t)& f_{1,3}(t)\\
    \vdots& \vdots& \vdots\\
    f_{k,1}(t)& f_{k,2}(t)& f_{k,3}(t)
  \end{bmatrix}^T
  \begin{bmatrix}
    w_1\\
    \vdots\\
    w_k
  \end{bmatrix},
\end{equation}
where $w_i$ is the associated weight for a specific flight $i$, that expresses its accuracy according to the set of all the collected flights, and: 
\begin{equation}\label{eq:mechanical-model-6}
  \begin{split}
    \mathbf{w}&\in\mathbb{R}^k,\\
    w_i&\in[0,1],\\
    w_1+w_2+\cdots+w_k&=1.
  \end{split}
\end{equation}
Based on a simple reordering, Equations~(\ref{eq:mechanical-model-4}--\ref{eq:mechanical-model-6}) can be also expressed:
\begin{equation}\label{eq:mechanical-model-7}
  \begin{split}
  \widetilde{\mathbf{f}}(t)=&
  \begin{bmatrix}
    {}^{1,1}a_0 & {}^{1,2}a_0 & {}^{1,3}a_0 \\
    {}^{1,1}b_0 & {}^{1,2}b_0 & {}^{1,3}b_0 \\
    \vdots & \vdots &\vdots \\
    {}^{1,1}a_3 & {}^{1,2}a_3 & {}^{1,3}a_3 \\
    {}^{1,1}b_3 & {}^{1,2}b_3 & {}^{1,3}b_3 \\
    {}^{2,1}a_0 & {}^{2,2}a_0 & {}^{2,3}a_0 \\
    {}^{2,1}b_0 & {}^{2,2}b_0 & {}^{2,3}b_0 \\
    \vdots & \vdots &\vdots \\
    {}^{k,1}a_3 & {}^{k,2}a_3 & {}^{k,3}a_3 \\
    {}^{k,1}b_3 & {}^{k,2}b_3 & {}^{k,3}b_3
  \end{bmatrix}^T
  \begin{bmatrix}
    w_1\cos\frac{0t}{\xi} \\ w_1\sin\frac{0t}{\xi} \\ \vdots \\ 
    w_1\cos\frac{3t}{\xi} \\  w_1\sin\frac{3t}{\xi} \\ 
    w_2\cos\frac{0t}{\xi} \\  w_2\sin\frac{0t}{\xi} \\ \vdots \\ 
    w_k\cos\frac{3t}{\xi} \\ w_k\sin\frac{3t}{\xi}
  \end{bmatrix}.
\end{split}
\end{equation}

Equation~(\ref{eq:mechanical-model-7}) can be used to derive all the constants for the three phases of the flight, and thus to describe the power consumption in the function of time for take-off, cruise, and landing. Table~\ref{tab:constants} outlines the constants that we obtained on the Opterra fixed-wing drone test flights of the agricultural use-case.

\begin{table}[h]
  \centering
  \caption{Constants for the mechanical energy model}
  \begin{tabular}{l|lll}
    \hline
     & take-off & cruise & landing \\\hline
    $a_0$ & 29.97   & 27.22   & 27.21\\
    $a_1$ & 1.57    & 0.3737  & -0.9512\\
    $b_1$ & -1.963  & 1.194   & 1.276\\
    $a_2$ & 0.3876  & 0.6513  & 0.9357\\
    $b_2$ & -1.552  & 0.2954  & 0.965\\
    $a_3$ & -0.2869 & 0.5039  & 0.4913\\
    $b_3$ & -0.547  & -0.2864 & -0.1192\\
    $\xi$ & 0.1525  & 0.1799  & 0.1296\\
    \hline
  \end{tabular}
  \label{tab:constants}
\end{table}

\subsection{\color{cyan}Fourier series of empirical data}
\label{sec:fourier}

Some collected energy data from the Opterra (in \fref{fig:opterra}{Figure}) adapted for precision agriculture scenario along its power spectrum back the choice of the periodic energy model. A periodic energy evolution of a fixed-wing aerial robot flying a plan and covering the polygon in \fref{fig:plot2}{Figure} (we describe the fixed-wing aerial robot plan in detail in \fref{sec:flight-plan}{Section}) is shown in \fref{fig:energy-1}{Figure}. \fref{fig:spectrum-1}{Figure} shows its frequency spectrum (the data are uniformly sampled). The spectrum is centered at zero frequency, which peaks at 4$\cdot$10${}^{\text{5}}$ and indicates the shift on the power-axis in \fref{fig:energy-1}{Figure}. The power spectrum shows that the energy evolution from \fref{fig:energy-1}{Figure} needs approximately three frequencies to be modeled. To obtain the spectrum in frequency space, we computed the Fourier transform.


\section{Periodic Energy Model}
\label{sec:periodic-model}

Let us suppose the aerial robot is operating in an autonomous scenario. Such a robot is often expected to iterate a set of tasks and paths periodically (see \fref{sec:outline}{Section}). For instance, the aerial robot might be covering a polygon such as the one proposed in \fref{sec:flight-plan}{Section}. Since the paths and tasks are periodically iterated over time, we expect the energy to evolve similarly. We will ease the assumption of the periodic evolution in practice to periodic with disturbance or aperiodic evolutions in \fref{sec:non-perio}{Section}. 
\begin{figure}[h!]
  \begin{subfigure}[b]{0.52\textwidth}
    \centering
    \footnotesize\fontfamily{phv}
    \input{figures/energy_1.tikz}
    \caption{Energy evolution}
    \label{fig:energy-1}
  \end{subfigure}\hspace{2mm}
  \begin{subfigure}[b]{0.46\textwidth}
    \centering
    \footnotesize\fontfamily{phv}
    \input{figures/spectrum_1.tikz}
    \caption{Power spectrum}
    \label{fig:spectrum-1}
  \end{subfigure}
  \caption[Energy data of a fixed-wing aerial robot]{Empirical energy data collected flying a fixed-wing aerial robot's plan and covering the polygon in \fref{fig:plot2}{Figure}. The data shows that the energy signal is periodic over time as the fixed-wing aerial robot reiterates a set of paths.}
\end{figure}
We motivate the choice of a periodic energy model further with some empirical energy data of the Opterra fixed-wing aerial robot flying the agricultural scenario in \fref{fig:energy-1}{Figure} and \fref{fig:spectrum-1}{Figure}.

In the remainder of this section, we first derive a differential periodic energy model and provide a formal proof in \fref{sec:deriv}{Section}. We enhance the model with the path and computations parameters in \fref{sec:nom-cont}{Section}, to predict the energy consumption of a given configuration of parameters. We explain how we convert the parameters into actual energy consumption in \fref{sec:merging}{Section}, and discuss the model's behavior for aperiodic energy evolutions in \fref{sec:non-perio}{Section}.

\subsection{Derivation of the differential periodic model}
\label{sec:deriv}

In the remainder of this section, we refer to the instantaneous energy consumption evolution simply as the energy signal. We model the energy using energy coefficients $\mathbf{q}\in\mathbb{R}^m$ that characterize such energy signal. The coefficients are derived from Fourier analysis (the size of the energy coefficients vector $m$ is related to the order of the Fourier series in \fref{sec:fourier}{Section}) and estimated using a state estimator in \fref{cp:est}{Chapter}. We prove a relation between the energy signal and the energy coefficients in \fref{lem:eqv}{Lemma}. %Later, we show how this approach allows us variability in terms of different evolutions, including aperiodic signals in \fref{sec:non-perio}{Section}. After illustrating the energy model, we enhance it with the energy contribution of the path and computations in \fref{sec:merging}{Section}. 

Let us consider a periodic energy signal of period $T$, and a Fourier series of an arbitrary order $r\in\mathbb{Z}_{\geq 0}$ for the purpose of modeling of the energy signal
\begin{equation}\label{eq:fourier}
  h(t)=a_0/T+(2/T)\sum_{j=1}^{r}{\left(a_j\cos{\omega jt}+b_j\sin{\omega jt}\right)},
\end{equation}
where $h:\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}$ maps time to the instantaneous energy consumption, $\omega:=2\pi/T$ is the angular frequency, and $a_0,a_j,b_j\in\mathbb{R}$ the Fourier series coefficients $\forall j\in[r]_{>0}$.

The energy signal can be modeled by \frefeq{eq:fourier} and by the output of a linear model
\begin{subequations}\label{eq:state-perf}\begin{align}
  \dot{\mathbf{q}}(t)&=A\mathbf{q}(t)+B\mathbf{u}(t),\\
  y(t)&=C\mathbf{q}(t)\label{eq:state-perf-output},
\end{align}\end{subequations}
where $y(t)\in\mathbb{R}$ is the instantaneous energy consumption. We discuss matrices $A,C,$ and $B$ in \frefeq{eq:mat_A}, \frefeq{eq:mat_C}, and \frefeq{eq:mat_B} respectively.

The state $\mathbf{q}(t)$ contains the energy coefficients
\begin{equation}\label{eq:state-details}
  \mathbf{q}(t)=\left[\begin{array}{cccccc}
    \alpha_0(t) & \alpha_1(t) & \beta_1(t) & \cdots & \alpha_r(t) & \beta_r(t)
  \end{array}\right]',\\
\end{equation}
where $\mathbf{q}(t)\in\mathbb{R}^m$ with $m=2r+1$. The state transition matrix
\begin{equation}\label{eq:mat_A}
  A=\left[\begin{array}{ccccc}
    0            & 0^{1\times 2}& 0^{1\times 2}& \dots& 0^{1\times 2} \\
    0^{2\times 1}& A_1          & 0^{2\times 2}& \dots& 0^{2\times 2} \\
    0^{2\times 1}& 0^{2\times 2}& A_2          & \dots& 0^{2\times 2} \\
    \vdots       & \vdots       & \vdots       &\ddots& \vdots        \\
    0^{2\times 1}& 0^{2\times 2}& 0^{2\times 2}& \dots& A_r 
  \end{array}\right],
\end{equation}
where $A\in\mathbb{R}^{m\times m}$. In matrix $A$, the top left entry is zero, the diagonal entries are $A_1,\dots,A_r$, the remaining entries are zeros. Matrix $0^{i\times j}$ is a zero matrix of $i$ rows and $j$ columns. The submatrices $A_1,A_2,\dots,A_r$ are defined
\begin{equation}
  A_j:=\begin{bmatrix}0 & \omega j \\ -\omega j & 0\end{bmatrix},
\end{equation}
$\forall j\in[r]_{>0}$. The output matrix
\begin{equation}\label{eq:mat_C}
  C=(1/T)\left[\begin{array}{cccccc}
    1 & 1 & 0 &\cdots & 1 & 0
  \end{array}\right],
\end{equation}
where $C\in\mathbb{R}^m$.

The linear model in \frefeq{eq:state-perf} allows us to include the control in the model of \frefeq{eq:fourier} as described in \fref{sec:nom-cont}{Section}. In the remainder we formally proof the equivalence and equality of the models in \frefeq{eq:fourier} and \frefeq{eq:state-perf}.

\begin{highlight}
\begin{lem}[Signal, output equality]\label{lem:eqv}Suppose control $\mathbf{u}$ is a zero vector, matrices $A,C$ are described by \frefeqM{eq:mat_A}{eq:mat_C}, and the initial guess $\mathbf{q}_0$ is 
  \begin{equation*}
  \mathbf{q}_0=\begin{bmatrix}a_0 & a_1/2 & b_1/2 & \cdots & a_r/2 & b_r/2\end{bmatrix}'.
  \end{equation*} 
  Then, the signal $h$ in \frefeq{eq:fourier} is equal to the output $y$ in \frefeq{eq:state-perf}.
\end{lem}
\end{highlight}

\begin{proof}
The proof justifies the choice of the items of the matrices $A,C$ and of the initial guess $\mathbf{q}_0$ in \frefeqM{eq:state-details}{eq:mat_C}. We write these elements such that the coefficients of the series $a_0,\dots,b_r$ are the same as the coefficients of the state $\alpha_0,\dots,\beta_r$.

Let us re-write the Fourier series expression in \frefeq{eq:fourier} in its complex form with the well-known Euler's formula 
\begin{equation}
  e^{it}=\cos{t}+i\sin{t},
\end{equation} 
where $i$ is the imaginary unit.

With $t=\omega jt$, we find the expression for 
\begin{subequations}\begin{align}
  \cos{\omega jt}&=(e^{i\omega jt}+e^{-i\omega jt})/2,\\  
  \sin{\omega jt}&=(e^{i\omega jt}-e^{-i\omega jt})/(2i),
\end{align}\end{subequations}
by substitution of $\sin{\omega jt}$ and $\cos{\omega jt}$ respectively. This leads~\citep{kuo1967automatic}
\begin{equation}\begin{split}\label{eq:proof-complex}
  h(t)=a_0/T+&(1/T)\sum_{j=1}^{r}{e^{i\omega jt}(a_j-ib_j)}+\\&(1/T)\sum_{j=1}^{r}{e^{-i\omega jt}(a_j+ib_j)}.
 \end{split}\end{equation} 

The solution at time $t$ of the model in \frefeq{eq:state-perf} under the assumptions in in the lemma (the control is a zero vector) can be expressed
\begin{equation}
  \mathbf{q}(t)=e^{At}\mathbf{q}_0.
\end{equation}

Both the solution and the system in \frefeq{eq:state-perf} are well-established expressions derived using standard textbooks~\citep{kuo1967automatic, ogata2002modern}. 

To solve the matrix exponential $e^{At}$, we use the eigenvectors matrix decomposition method~\citep{moler2003nineteen}.

The method works on the similarity transformation 
\begin{equation}
  A=VDV^{-1}.
\end{equation}
The power series definition of $e^{At}$ implies~\citep{moler2003nineteen}
\begin{equation}
  e^{At}=Ve^{Dt}V^{-1}.
\end{equation} 

We consider the non-singular matrix $V$, whose columns are eigenvectors of $A$ 
\begin{equation}
  V:=\begin{bmatrix}v_0 & v_1^0 & v_1^1 & \dots & v_r^0 & v_r^1\end{bmatrix}.
\end{equation}

We then consider the diagonal matrix of eigenvalues 
\begin{equation}
  D=\mathrm{diag}{(\lambda_0,\lambda_1^0,\lambda_1^1,\dots,\lambda_r^0,\lambda_r^1)}.
\end{equation}
$\lambda_0$ is the eigenvalue associated to the first item of $A$. $\lambda_j^0,\lambda_j^1$ are the two eigenvalues associated with the block $A_j$. We can write 
\begin{equation}
AV=VD.
\end{equation}

We apply the approach in terms of \frefeq{eq:state-perf} (under the assumptions made in the lemma) 
\begin{equation}
  \dot{\mathbf{q}}(t)=A\mathbf{q}(t).
\end{equation}
The linear combination of the initial guess and the generic solution
\begin{subequations}\begin{align}
  F\mathbf{q}(0)&=\gamma_0 v_0+\sum_{k=0}^{1}{\sum_{j=1}^{r}{\gamma_j v_j^k}},\\
  F\mathbf{q}(t)&=\gamma_0 e^{\lambda_0 t} v_0+\sum_{k=0}^{1}{\sum_{j=1}^{r}{\gamma_j e^{\lambda_j t} v_j^k}},\label{eq:proof-comb}
\end{align}\end{subequations}
where 
\begin{equation}
  F=\begin{bmatrix}1 & \cdots & 1\end{bmatrix},
\end{equation} 
$F\in\mathbb{R}^m$ is a column vector of ones. 

Let us consider the expression in \frefeq{eq:proof-comb}. It represents the linear combination of all the coefficients of the state at time $t$. It can also be expressed in the following form
\begin{equation}\label{eq:proof-output}\begin{split}
  F\mathbf{q}(t)/T=\gamma_0 e^{\lambda_0t}v_0/T+&(1/T)\sum_{j=1}^r{\gamma_j e^{\lambda_j^0t}v_j^0}+\\&(1/T)\sum_{j=1}^r{\gamma_j e^{\lambda_j^1t}v_j^1}.
\end{split}\end{equation}

We prove that the eigenvalues $\mathbf{\lambda}$ and eigenvectors $V$ are such that \frefeq{eq:proof-output} is equivalent to \frefeq{eq:proof-complex}.

The matrix $A$ is a block diagonal matrix, so we can express its determinant as the multiplication of the determinants of its blocks
\begin{equation}
  \det{(A)}=\det{(0)}\det{(A_1)}\det{(A_2)}\cdots\det{(A_r)}.
\end{equation}

We now conclude the proof by computing the first determinant and the others separately.

By computing the first determinant, we prove that the first terms in \frefeq{eq:proof-complex} and \frefeq{eq:proof-output} match. We find the eigenvalue from $\det(0)=0$, which is $\lambda_0=0$. The corresponding eigenvector can be chosen arbitrarily
\begin{equation}\label{eq:proof-first-det}
  (0-\lambda_0)v_0=\begin{bmatrix} 0 & \cdots & 0 \end{bmatrix},
\end{equation}
$\forall v_0$, thus we choose
\begin{equation}\label{eq:proof-v0}
  v_0=\begin{bmatrix}1 & 0 & \cdots & 0\end{bmatrix}.
\end{equation}
The sizes of the zero vector and of $v_0$ in \frefeqM{eq:proof-first-det}{eq:proof-v0} are both $\mathbb{R}^m$.

We find the value $\gamma_0$ in \frefeq{eq:proof-output} so that the terms are equal 
\begin{equation}
  \gamma_0=\begin{bmatrix}a_0 & 0 & \cdots & 0\end{bmatrix},
\end{equation} 
where $\gamma_0\in\mathbb{R}^m$.

Then, we prove the other determinants. In this way, we prove that all the terms in the sum of both \frefeq{eq:proof-complex} and \frefeq{eq:proof-output} match. 

By computing the second determinant, we prove that the first terms in both summaries in \frefeq{eq:proof-complex} and \frefeq{eq:proof-output} match. We thus focus on the first block $A_1$, and find the eigenvalues from 
\begin{equation}
  \det(A_1-\lambda I)=0.
\end{equation}
The polynomial $\lambda^2+\omega^2$, gives two complex roots--the two eigenvalues
\begin{subequations}\begin{align}
  \lambda_1^0&=i\omega,\\
  \lambda_1^1&=-i\omega.
\end{align}
\end{subequations}

The eigenvector associated with the eigenvalue $\lambda_1^0$ is 
\begin{equation}
  v_1^0=\begin{bmatrix}0 & -i&1&0&\cdots&0\end{bmatrix}'.  
\end{equation}

The eigenvector associated with the eigenvalue $\lambda_1^1$ is 
\begin{equation}
  v_1^1=\begin{bmatrix}0&i&1&0&\cdots&0\end{bmatrix}'. 
\end{equation}
Both eigenvectors are equally sized $v_1^0,v_1^1\in\mathbb{R}^m$.

Again, we find the values $\gamma_1$ in \frefeq{eq:proof-output} such that the equivalences 
\begin{equation}\begin{cases}    
  e^{i\omega t}(a_1-ib_1)&=\gamma_1 e^{i\omega t}v_1^0\\
  e^{-i\omega t}(a_1+ib_1)&=\gamma_1 e^{i\omega t}v_1^1
\end{cases},\end{equation}
hold. They hold for 
\begin{equation}
  \gamma_1=\begin{bmatrix}0&b_1&a_1&0&\cdots&0\end{bmatrix}.
\end{equation} 

The proof for the remaining $r-1$ blocks is equivalent.

The initial guess $\mathbf{q}_0$ is build such that the sum of the coefficients is the same in both the signals. In the output matrix, the frequency $1/T$ accounts for the period in \frefeq{eq:proof-complex}, \frefeq{eq:proof-output}, and~\frefeq{eq:fourier}. At time instant zero, the coefficients $b_j$ are not present and the coefficients $a_j$ are doubled for each $j=1,2,\dots,r$ (thus we multiply by a half the corresponding coefficients in $\mathbf{q}_0$). To match the outputs $h(t)=y(t)$, or equivalently 
\begin{equation}
  F\mathbf{q}(t)/T=C\mathbf{q}(t), 
\end{equation}
we define 
\begin{equation}
  C:=(1/T)\begin{bmatrix}1 & 1 & 0 & \cdots & 1 & 0\end{bmatrix}.
\end{equation}

We thus conclude that the signal and the output are equal and that the lemma holds.

\end{proof}

We note for practical reasons that the signal would still be periodic with another linear combination of coefficients. For instance
\begin{equation}\label{eq:mat_C_generic}
  C:=d\begin{bmatrix}1 & 1 & 0 & \cdots & 1 & 0\end{bmatrix},
\end{equation} 
equivalent to
\begin{equation}
  C:=d\begin{bmatrix}1 & 0 & 1 & \cdots & 0 & 1\end{bmatrix},
\end{equation} 
or 
\begin{equation}
  C:=d\begin{bmatrix}1 & \cdots & 1\end{bmatrix},
\end{equation} 
for a constant value $d\in\mathbb{R}$.

\subsection{Nominal control}
\label{sec:nom-cont}

Let us suppose that at time instant $t$ the plan in \fref{def:plan}{Definition} reached the $i$th stage $\Gamma_i$ and the control contains the configuration of path and computations parameters 
\begin{equation}\label{eq:state-control2}
  c_i(t)=\begin{bmatrix}c_i^\rho(t) & c_i^\sigma(t)\end{bmatrix}',
\end{equation}
where $c_i(t)\in\mathbb{R}^n$ with $n=\rho+\sigma$ differs from the nominal control $\mathbf{u}(t)$ in \frefeq{eq:state-perf}. We include the control in the nominal control exploiting the following observation. 

\begin{highlight}
  \begin{obs}
    We observe that:
    \begin{itemize}
      \item A change in path parameters affects the energy indirectly. It alters the time when the aerial robot reaches the final point $\mathbf{p}_{\Gamma_l}$, and enters the accepting stage $\Gamma_f$.
      \item A change in computation parameters affects the energy directly. It alters the instantaneous energy consumption as more computations require more power (and vice versa).
    \end{itemize}
  \end{obs}
\end{highlight}

%\begin{figure}[p]
%  \centering
%  \fontfamily{phv}\selectfont
%  \input{figures/plot8.tikz}
%  \caption[Trajectory of the aerial robot flying the highest path configuration]{The trajectory of the aerial robot flying paths $\varphi_1,\varphi_2,\dots,\varphi_l$ from \fref{sec:flight-plan}{Section} with the highest configuration of the path parameter $c_{4,1}=\overline{c}_{4,1}$. This configuration of path parameter has the longest time but the shortest distance between the lines and thus the highest quality of the coverage in \fref{fig:plot2}{Figure}.}
%  \label{fig:plot-8}
%\end{figure}
%\begin{figure}[p]
%  \centering
%  \fontfamily{phv}\selectfont
%  \input{figures/plot9.tikz}
%  \caption[Trajectory of the aerial robot flying the lowest path configuration]{The trajectory of the aerial robot similarly as in \fref{fig:plot-8}{Figure} but with the lowest configuration of the path parameter $c_{4,1}=\underline{c}_{4,1}$. This configuration has the longest distance between the lines and thus the lowest quality of the coverage in \fref{fig:plot2}{Figure}.}
%  \label{fig:plot-9}
%\end{figure}

The second bullet point in the observation is easily verified. The \powprof{} profiling tool models the energy consumption of the heterogeneous computing hardware the mobile robot is caring. A variation in the computation parameter affects the schedule (as the schedule is parametrized by the computation parameter in \fref{def:comp-mot-energy}{Definition}) and hence the instantaneous energy consumption of the mobile robot.

The first bullet point in the observation can be verified by inspection of the example in \fref{sec:flight-plan}{Section}. It is clear that if we decrease the parameter $c_{4,1}$, which is relative to the radius of the circle, the flying time decreses. This is further shown in \fref{fig:zambo1}{Figure} and \fref{fig:zambo2}{Figure}. \fref{fig:zambo1}{Figure} illustrates the trajectory of the aerial robot (composed of all the paths $\varphi_1,\varphi_2\,\dots$) flying at the highest configuration of the path parameter $c_{i,1}=\overline{c}_{4,1}$. \fref{fig:zambo2}{Figure} then illustrate the trajectory flying at the lowest configuration $c_{i,1}=\underline{c}_{4,1}$. The flying time differs significantly, along with the quality of the coverage of the polygon (the agricultural field in \fref{fig:plot2}{Figure}). In \fref{fig:zambo2}{Figure}, the parameter $c_{4,1}$ that alters the radius and center of the upper circle (defined originally in \fref{sec:flight-plan}{Section}) is replanned as, e.g., averse atmospheric conditions do not allow to terminate the original plan in \fref{fig:zambo1}{Figure}.

We use the observation later in \fref{sec:opt-cont-gener}{Section} to check that the time to completely discharge the battery is greater than the flight time and replan the path parameters accordingly.  We replan the computation parameters to maximize the instantaneous energy consumption against the maximum battery discharge rate.

The nominal control is
\begin{equation}\label{eq:state-control}
  \mathbf{u}(t):=\hat{\mathbf{u}}(t)-\hat{\mathbf{u}}(t-\Delta t),
\end{equation}
where $\hat{\mathbf{u}}(t)$ is defined as the energy estimate of a given control sequence at time instant $t$, $\hat{\mathbf{u}}(t-\Delta t)$ at the previous time instant $t-\Delta t$
\begin{equation}\label{eq:estimate-control}
  \hat{\mathbf{u}}(t):=\mathrm{diag}(\nu_i)c_i(t)+\tau_i,
\end{equation}
where $\mathrm{diag}(\nu_i)$ is a diagonal matrix with the parameters
$\nu_{i,j}\in\nu_i,\,\forall j\in[n]_{>0}$.

The input matrix is then
\begin{equation}\label{eq:mat_B}
  B=\begin{bmatrix} 
    0^{1\times\rho} & 1      & \cdots & 1      \\
    0^{1\times\rho} & 0      & \cdots & 0      \\ 
    \vdots          & \vdots & \ddots & \vdots \\
    0^{1\times\rho} & 0      & \cdots & 0      \end{bmatrix},
\end{equation}
where $B\in\mathbb{R}^{m\times n}$ contains zeros except the first row where the first $\rho$ columns are still zeros and the remaining $\sigma$ are ones. 

$\hat{\mathbf{u}}(t)$ is a stage-dependent scale transformation with 
\begin{subequations}\label{eq:scaling}\begin{align}
\nu_i&=\begin{bmatrix}\nu_i^\rho & \nu_i^\sigma\end{bmatrix}',\\ 
\tau_i&=\begin{bmatrix}\tau_i^\rho & \tau_i^\sigma\end{bmatrix}',
\end{align}\end{subequations}
scaling factors. They quantify the contribution to the plan of a given parameter in terms of time for the first $\rho$ parameters, and instantaneous energy consumption for the remaining $\sigma$ (we use the same notation for the path and computation scaling factors as for the parameters). 

The nominal control $\mathbf{u}(t)$ is then the difference of these contributions of two consecutive controls $c_i(t-\Delta t),c_i(t)$ applied to the system. 

$B\mathbf{u}(t)$ merely includes the difference in the instantaneous energy consumption into the model in \frefeq{eq:state-perf}. Matrix $B$ ignores the time contribution of the path parameters in $c_i$. We use them to verify that the flying time is lower than the battery time in \fref{sec:algo}{Section}.

\subsection{Parameters scale transformation}
\label{sec:merging}

%\begin{figure}[h]
%  \centering
%  \input{figures/traj4.tikz}
%  \caption[Path parameter in the flight of an aerial robot]{A path parameter in the flight of an aerial robot can be used to alter the overall energy consumption}
%  \label{fig:tee1}
%\end{figure}
%\begin{figure}[h]
%  \centering
%  \input{figures/traj2.tikz}
%  \caption[Alteration of the path parameter]{The alteration of the path parameter $c_{1,1}$, the radius of the circle.}
%  \label{fig:tee2}
%\end{figure}

%Equation~(\ref{eq:state-control}) accounts for the energy due to the change of parameters $\mathbf{u}_k-\mathbf{u}_{k-1}$. For instance, when the trajectory $\varphi_1$ is a circle (see Figure~\ref{fig:tee1}), a decrement in the trajectory parameter $c_{1,1}$--the radius of the circle--adds a negative contribution. It thus simulates the lowering of instantaneous energy consumption ($\nu_{1,1}c_{1,1}>\nu_{1,1}c_{1,1}^-$) for a given $\nu_{1,1}$, that is then summed to the first coefficient $\alpha_0$ in Equation~(\ref{eq:state-details}), shifting the modeled energy.

To transform the control $c_i(t)$ at $i$th stage and time instant $t$ we use different approaches for the path and computation scaling factors.

The scaling factors for the path parameters from \frefeq{eq:scaling} are derived empirically. For example, we can obtain the scaling factor $\nu_{4,1}$ relative to the alteration $c_{4,1}$ of the upper circle $\varphi_4$ from \fref{sec:flight-plan}{Section} by measuring the time needed to compute the path with the lowest configuration $\underline{c}_{4,1}$, $\underline{t}$ in \fref{fig:zambo2}{Figure}, and the highest $\overline{t}$ in \fref{fig:zambo1}{Figure}. 

The variation of the control hence results in an approximate measure of the plan's time variation with factors
\begin{subequations}\label{eq:scale-traj}\begin{align}
  \nu_{i,j}&=\left((\overline{t}-\underline{t})/(\overline{c}_{i,j}-\underline{c}_{i,j})\right)/\rho,\\
  \tau_{i,j}&=\left(\underline{c}_{i,j}(\underline{t}-\overline{t})/(\overline{c}_{i,j}-\underline{c}_{i,j})+\underline{t}\right)/\rho,
\end{align}\end{subequations} 
$\forall j\in[\rho]^+$. Moreover, let the factors be zero when the parameters set $c_i^\rho=\{\emptyset\}$. We use the latter to initialize the algorithm in \fref{sec:algo}{Section}.

%Whenever the trajectory parameters are not equally distributed, one can define $(y_{\overline{c}_i}-y_{\underline{c}_i})$ as a the highest (and lowest) levels of specific trajectory parameters. 

The scaling factors for the computations parameters from \frefeq{eq:scaling} are derived using {\small\tt{powprofiler}}, the open-source modeling tool from \fref{sec:powprof}{Section}. We estimate the energy cost of a given schedule (a given computations configuration) with the function $g$ from \fref{def:comp-ener}{Definition}. 
%adapted from earlier work on computational energy analysis~\citep{seewald2019coarse, seewald2019component}, and energy estimation of a fixed-wing UAV~\citep{seewald2020mechanical}. 
%For this purpose, we assume the mobile robot carries an embedded board that runs the computations. Our
%In both cases, with ROS~\citep{zamanakos2020energy} or with generic software components system~\citep{seewald2019component}, the tool performs automatic modeling. 
For instance, if the computation is the CNN ROS node, the computation parameter $c_{1,2}$ corresponds to the \Gls{acr:fps} rate. The tool then measures power according to the detection frequency.

The scaling factors add the computational energy component to the model in \frefeq{eq:state-perf}. They are derived similarly to \frefeq{eq:scale-traj}
\begin{subequations}\label{eq:scale-comp}\begin{align}
  \nu_{i,j}&=(g(\overline{c}_{i,j})-g(\underline{c}_{i,j}))/(\overline{c}_{i,j}-\underline{c}_{i,j}),\\
  \tau_{i,j}&=\underline{c}_{i,j}(g(\underline{c}_{i,j})-g(\overline{c}_{i,j}))/(\overline{c}_{i,j}-\underline{c}_{i,j})+g(\underline{c}_{i,j}),
\end{align}\end{subequations}
$\forall j\in[\rho+1,n]$. Moreover, let the factors be zero when the parameters set $c_i^\sigma=\{\emptyset\}$.

The concept of a path and a computation parameter scale transformation is illustrated in \fref{fig:plot-6}{Figure}. 
\begin{figure}[h]
  \centering
  \fontfamily{phv}\selectfont
  \input{figures/plot6.tikz}
  \caption[Concept of a path and computation parameter scale transformation]{The concept of a path and computation parameter scale transformation. Without any battery constraints, the dynamic energy planning selects the highest configuration which respects the control constraint (inner rectangle) from \frefeq{eq:constraint-set}.}
  \label{fig:plot-6}
\end{figure}
The energy domain is bounded by the output of \powprof{}, while the flight time domain by the empirical data. The dynamic energy planning selects the highest possible configuration under the constraints. Currently, the highest configuration of parameters corresponds to the highest configuration of parameters $(\overline{c}_{i,1},\overline{c}_{i,2})$ (blue point in the figure). We will show in \fref{sec:output-mpc}{Section} the optimal control derivation over a time horizon $N$ under given battery constraints.
\begin{figure}[h]
  \centering
  \fontfamily{phv}\selectfont
  \input{figures/plot7.tikz}
  \caption[.]{.}
  \label{fig:plot-7}
\end{figure}
%todo explain a bit plot7 

\subsection{\color{red}Aperiodic energy evolution}
\label{sec:non-perio}

\section{\color{red}Contribution}


\section{\color{cyan}Results}

This Section shows and assesses experimental results for the benchmarks, and validates the presented approach.

We now describe the experimental results of the benchmarks previously introduced. A summary of these results is outlined in \fref{tab:benchmark-components}{Table}.

\begin{table}[h]
  \centering
  \begin{tabular}{l|*{3}{c|}c}
    \hline
    \multirow{2}{*}{Component} & ODROID & \multicolumn{3}{c}{NVIDIA} \\
    & XU3 & TK1 & TX2 & Nano \\
    \hline
    \stt{matrix-cpu}    & 5284 J & 4067 J & 2413 J & 2736 J \\
    \stt{matrix-gpu}    & - & 81 J & 45 J & 39 J \\
    \stt{darknet-cpu}   & (-) & (-) & 2400 J & (-) \\
    \stt{darknet-gpu}   & - & - & 5255 J & (-) \\
    \stt{nvidia-matrix} & - & (-) & 4054 J & (-) \\
    \stt{nvidia-quicks} & - & (-) & 1995 J & (-) \\
    \hline
  \end{tabular}
  \caption{The overall energy consumption for each benchmark. Unsupported platforms are indicated by `-' and `(-)' indicates supported but not included in this paper.}
  \label{tab:benchmark-components}
\end{table}

\subsubsection*{\color{cyan}Matrix Exponentiation}
\label{sec:experimental-results:matrix}

Execution of GPU matrix exponentiation, while varying size and exponent parameters, was previously shown in \fref{fig:matrix-exponent}{Figure}. The figure shows the average power as well as overall energy consumption along with the battery depletion as a function of size and exponent parameters. Average power consumption is reported independently of the running time of the component and thus does not reflect the total power consumption. For small problem sizes, the computation terminates before reaching the maximal power level. This effect is visible in \fref{fig:darknet-layer1}{Figure} (also previously shown), where power consumption is low at the beginning and then reaches the maximum, for which reason the average power consumption is low for small problem sizes. Battery depletion is reported in terms of the total amount of energy consumed by the computation for the duration of the execution. The effect of introducing ``scheduling'' in the form of sleep of various durations in between iterations of the matrix computations can be seen in \fref{fig:matrix-sleep}{Figure}. Here, the duration of the sleep affects the total power consumption: the higher the sleep value in between the iterations, the greater the battery depletion.

CPU vs GPU comparison shows, expectedly, that \stt{matrix-gpu} is very performant compared to \stt{matrix-cpu}. While the \stt{matrix-cpu} requires 2~413~J, \stt{matrix-gpu} requires only 45~J for the same operation on the TX2 board. Therefore, running the benchmark on GPU results in 16\% more SoC against the same trial on CPU. Such a high speed-up is observed due to the highly parallelizable nature of the matrix exponentiation and hence cannot be used as a general rule. From our experiments, we additionally observe the power-related effect of running components sequentially versus in parallel on different computational units. Although the CPU and GPU are different computational units, the energy consumed by running components independently (i.e., sequentially in some order) on CPU and GPU is 20\% larger compared to running them in parallel, even when subtracting the base power consumed by the board. Thus %the energy cost of components executing independently on different computational units cannot simply be summed to obtain the energy cost of running them in parallel.
energy can be conserved by running computations in parallel on the CPU and GPU, compared to scheduling them sequentially.


\subsubsection*{\color{cyan}Darknet}
\label{sec:experimental-results:darknet}

We modified the \stt{darknet} component to simulate different scheduling options and evaluated the outcome on a video stream: \stt{darknet} now %. A first modification of the source 
accepts an argument that indicates the amount of sleep between two invocations of the image recognition algorithm. In this way, we were able to simulate different frames-per-second options and assess the power evolution using the model. %Furthermore, the source was extended to support different batching options. Several frames are collected in a queue of defined size so that the image recognition phase process them in batch. In this fashion, a soft-realtime approach can be investigated, for example simulating a requirement that 10 images should be processed every 10 seconds. Evaluating the impact of such scheduling variations is however considered future work. %UPS: having it implemented by having no experiments with this makes no sense, if powprofiler works and the option works, it shouldn't take more than half a day to do this - so let's not mention it for now.

\begin{figure}[t]
  \centering
  %\input{\figpath/darknet-layer2.tikz}
  \caption[Layer 2 model of \stt{darknet-gpu} component]{Layer 2 model of \stt{darknet-gpu} component running under four configurations, respectively 5.8, 10, 25 and 32 frames-per-second. The figure shows the per-minute energy consumption in terms of CPU, GPU, and overall. On the right side, energy consumption for any possible frame per second rate is shown along with SoC (the y2-axis, the y-axis is shared with the left side).}
  \label{fig:darknet-layer2}
\end{figure}

%Data obtained from the first modification (see Figure~\ref{fig:darknet-layer2}), 
Our experimental data depicted in Figure~\ref{fig:darknet-layer2} shows that an increment in frames-per-second corresponds to an increased power consumption along with a higher battery depletion. The resulting model can be used to define an appropriate trade-off that represents an acceptable rate of QoS, by i.e., correlating the FPS rate to the battery depletion and hence highlighting the dynamic behavior of a mobile scenario.
%A later trial seems to highlight  %UPS: confusing wording
Moreover, we observe that \stt{darknet-cpu} consumes less energy per minute compared to the \stt{darknet-gpu} component, but is considerably slower: while running \stt{darknet-cpu} for one minute on TX2 board requires 2~400~J, \stt{darknet-gpu} requires 5~255~J. When considering the energy cost per frame the computation is however considerably more efficient on GPU, where it requires just 1.3~J per frame, against the 175~J on GPU. %UPS: missing how many fps was it running on GPU and CPU?

\subsubsection*{\color{cyan}NVIDIA}

%Matrix multiplication (\stt{nvidia-matrix}) and quicksort (\stt{nvidia-quick}) benchmarks from NVIDIA were included in this paper for reproducibility. %UPS: there's no reproducibility since we don't provide any concrete numbers.
The \stt{nvidia-matrix} benchmark differs from \stt{matrix-gpu}, even if both perform matrix multiplication on GPU, since \stt{nvidia-matrix} includes a significant CPU computation after GPU matrix multiplication to check whether the two match. The \stt{matrix-gpu} benchmark was similarly tested during development, but does not perform this test at runtime. We observe that the \stt{nvidia-matrix} benchmark has a similar energy behavior to \stt{matrix-gpu}, with the problem size affecting the overall energy and henceforth battery depletion. Average power consumption on GPU is however higher for \stt{matrix-gpu} while it is approximately the same on CPU for both benchmarks (presumably none of the two benchmarks focus on energy efficiency on CPU). For the quicksort benchmark, as expected energy consumption increases with the problem size, as more operations are performed. Nevertheless quicksort differs from matrix multiplication: %its evolution includes some
there is more noise due to a higher dependence on the random data that is being sorted. %The noise is almost unvisible in matrix multiplication, as the benchmark emphasizes the validity of the multiplication on GPU (multiplication operation itself is rather fast even rising considerably problem size that in the current implementation is limited), thus the fact that the two matrixes are generated randomly affects only marginally the model. 

\subsection{\color{cyan}Validation}
\label{sec:experimental-results:validation}

We validate our approach by: 
\begin{enumerate*}[label={\alph*)},font={\bfseries}]
  \item demonstrating that \powprof{} can be used on a number of heterogeneous platforms,
  \item comparing model generated with internal metrics to external physical measurements on the TX2, and
  \item comparing the model to a fine-grained one.
\end{enumerate*}

A cross-platform comparison shows energy-related behavior of running the same benchmark on different boards. We observed, for instance, that the most energy-efficient board for \stt{matrix-cpu} benchmark is TX2, which consumes 2~413~J to perform the operation, followed by Nano with 2~736~J, TK1 with 4~067~J, and ODROID with 5~284~J.

A %detailed %UPS: I don't think we're "detailed" here...
measurement analysis is used to compare internal against external power metrics on the TX2 board. We observe that both externally and internally measured power models are close one to each other, with an error of less than 3\% measured over one minute. The externally measured power exceeds the internally measured power --- this is natural as the externally measured power also includes the carrier board, which requires additional energy to operate. Moreover, the measurements performed using \powprof{} include the power consumed by \powprof{} itself, while the multimeter setup excludes \powprof{}'s energy from the evaluation. We therefore assume that the tool has a marginal effect on power consumption and that the model is showing realistic behavior.

In a last validation step, we compared our approach to fine-grained energy model of Nunez et al.~\citep{nunez2013enabling} and Nikov et al.~\citep{nikov2015evaluation} for the ODROID-XU3 embedded board. To relate the two approaches for energy-modeling, we used the \stt{matrix-cpu} benchmark component as follows. First, we evaluated the matrix exponentiation benchmark by raising a 512 by 512 matrix to the 30th power ($512^{30}$) to train the fine-grained model. We obtained a value of expected energy per operation that we compared to the measured one and measured a relative error of 3.42\%. Second, we applied an equivalent approach to our model. We sampled some configurations that were distant from the expected one, concretely we ran configurations $512^{x}$ with $x\in \{5,15,25,35,\dots\}$, and we used the approximation method described in \fref{sec:predictive-model:approximation-methos}{Section} to evaluate the energy of the configuration $512^{30}$. This value was then compared to the measured one and we obtained a relative error of 2.25\%. We can conclude that both models produce similar results on this benchmark, and have a similar relative error even if they adopt a different approach towards energy-modeling.

\subsection{\color{cyan}Assessment}
\label{sec:experimental-results:assessment}

We performed a number of experiments on different boards. Most of the data in this paper were obtained from NVIDIA TX2 due to the similarity with TK1 and Nano and the easy accessibility of the internal power measurement units. These data allowed us to validate our model, and to show that different scheduling options can correspond to different energy models. The model can describe energy consumption and instantaneous power, together with the battery depletion.

Coarse-grained whole-system energy modeling can take into account some behaviors that cannot otherwise easily be observed. % by other models.% UPS: begs the question what models, safer to leave that out
As an example, consider the effect of simple scheduling previously shown in \fref{fig:matrix-sleep}{Figure}. Here, we expected that introducing a higher sleep period between executions would result in an energy cost. We used the model to investigate this assumption, and we found that it is not always happening. For instance, a period of 2.5s between two consecutive schedules of $512^{12}$ matrix exponentiation iterated for 12 times is more energy-efficient than executing the whole $512^{144}$ operation. The model can relate these observations, provide information about the battery depletion, and predict the total time the system can operate on a given energy source. It can also highlight battery-specific behaviors since different scheduling options drain battery differently. In particular, Using the \powprof{} tool on the NVIDIA~TX2, we experimentally observe that:
\begin{itemize}
  \item Running a set of components separately, and simply adding their energy consumption (while excluding base energy), leads to a different model versus running them in parallel (\fref{sec:experimental-results:matrix}{Section}). This behavior was observed for matrix exponentiation components running on separate computational units (CPU, GPU).
  \item Scheduling of computations directly impacts the battery drain: processing a computation in its entirety with a steady power load drains the battery less than scheduling that same computation into smaller steps with resulting spikes in the power load (\fref{sec:experimental-results:assessment}{Section}). This behavior was observed for a matrix exponentiation component running on CPU.
\end{itemize}


\section{\color{red}Summary}

