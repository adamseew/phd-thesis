
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                              %
% Optimal Control Generation   %
%                              %
\chapter{Optimal Control Generation}
\label{cp:opt}

%\begin{highlight}
%    \begin{st}
%        Initial version up to \fref{cp:opt:unconstrained}{Subsection}. From \fref{sec:opt-constrained}{Subsection} dummy text.
%    \end{st} 
%\end{highlight}

\lettrine{T}{his} chapter provides essential theoretical background on optimal control theory necessary to derive an optimal configuration of the path and computations of the mobile robot. It solves the problem posed in~\fref{sec:pb-form}{Section} and illustrate an algorithm that generates the optimal configuration dynamically. The algorithm relies on a modern optimal control technique known as model predictive control (\Gls{acr:mpc}), where the optimal control trajectory is evaluated on a receding horizon for each optimization step~\citep{rawlings2017model}. 

Optimal control deals with finding optimal ways to control a dynamic system~\citep{sethi2019optimal}. It determines the control signal--the evolution in time of the decision variables--such that the model satisfies the dynamics and simultaneously optimizes a performance index~\citep{kirk2004optimal}.

Many optimization problems originating in fields such as robotics, economics, and aeronautics can be formulated as optimal control problems~(\Gls{acr:ocp}s)~\citep{von1992direct}. Optimization is often called mathematical programming~\citep{nocedal2006numerical} a term that means finding ways to solve the optimization problem. One can often find programming in this context in terms such as linear program (\Gls{acr:lp}), quadratic program (\Gls{acr:qp}), and nonlinear program (\Gls{acr:nlp}). \Gls{acr:nlp}s is the class of optimization problems that we use to derive the optimal configuration. \Gls{acr:ocp}s can be seen as optimization problems with the added difficulty of continuous dynamics. The latter is to be integrated over the optimization horizon using numerical simulation. In the algorithm, we formulate the dynamic planning problem as an \Gls{acr:ocp} that we solve with a numerical method: we transform the \Gls{acr:ocp} in an \Gls{acr:nlp} using numerical simulation and solve the \Gls{acr:nlp} using numerical optimization, as proposed in~\citep{rawlings2017model}. The process is illustrated in \fref{fig:opt:process-summary}{Fig.}.

\begin{figure}[!h]
    \centering
    \footnotesize
    \begin{tikzpicture}[auto, node distance=3.2cm,>=latex']
        \node [input, name=rinput] (rinput) {};
        \node [right of=rinput] (mobrob) {\input{figures/opterra-bp.tikz}};
        \node [block,right of=mobrob,node distance=2.2cm] (theblock) {\Gls{acr:ocp}};
        \node [block,right of=theblock,node distance=2.2cm] (theblock2) {\Gls{acr:nlp}};
        \draw [->] (mobrob) -- node{} (theblock);
        \draw [->] (theblock) -- node{} (theblock2);
        \draw [->] (theblock2) -- ++(0,-1cm) -| node[pos=0.23] {path/computations} 
        node [near end] {} (mobrob);
        \draw [decorate,decoration={brace,amplitude=5pt,raise=26pt},yshift=0pt] (theblock.north west) -- (theblock2.north east) node [black,midway,yshift=10ex]{numerical simulation};

        \draw [decorate,decoration={brace,amplitude=5pt,raise=4pt},yshift=0pt] (theblock2.north west) -- (theblock2.north east) node [black,midway,yshift=3ex]{numerical optimization};
        \draw [decorate,decoration={brace,amplitude=5pt,raise=4pt},yshift=0pt] (mobrob.north west) -- (mobrob.north west -| theblock.north east) node [black,midway,sloped,yshift=3ex]{dynamic planing problem};
    \end{tikzpicture}
    \caption[Summary of the optimal control approach]{Summary of the optimal control approach. The problem is formulated as an \Gls{acr:ocp}, into finite-dimensional discrete \Gls{acr:nlp} using numerical simulation. \Gls{acr:nlp} is solved using numerical optimization and the optimal configuration for a given time horizon is returned to the aerial robot. The following horizon is evaluated again in a technique known as \Gls{acr:mpc}.}
    \label{fig:opt:process-summary}
\end{figure}

A typical performance measure for an \Gls{acr:ocp} is built such that the system: reaches a target set $\mathcal{Q}_f$ in minor time, reaches a given final state $\mathbf{q}_f$ with minimum deviation, maintains the state evolution as close as possible to a given desired evolution, or reaches the target set with the minimum control expenditure effort~\citep{kirk2004optimal}. In energy planning, it is desired to focus on the latter performance measure. 

The outline of the chapter is as follows. After a brief history of optimal control, we introduce formally the \Gls{acr:ocp} subject to continuous dynamics. We then illustrate numerical simulation approaches to convert the infinite-dimensional continuous dynamics into finite-dimensional discrete dynamics. We formulate later in the chapter the dynamic planning problem for the optimal configuration of the path and computations with proper constraints. Finally, we illustrate \Gls{acr:mpc} to solve \Gls{acr:ocp} on a receding horizon. We propose an algorithm to solve such \Gls{acr:ocp} using a numerical method on the horizon along with the analysis of its practical feasibility.

The chapter builds on the rest of the work as follows. In the \Gls{acr:ocp} formulation, we use the estimated state from \fref{cp:est}{Chapter} of a perfect model in \fref{cp:model}{Chapter} to solve the planning problem in \fref{sec:pb-form}{Section} and guide the aerial robot with the obtained optimal configuration from this chapter with the technique in \fref{cp:gd}{Chapter}.

\section{A Brief History of Optimal Control}

Optimal control originates from the calculus of variations~\citep{sethi2019optimal}, based on the work of Euler and Lagrange. Calculus of variations solves the problem of determining the arguments of an integral, such that its value is maximum (or minimum). The equivalent problem in calculus is to determine the argument of a function where the function is maximum (or minimum). The work by Euler and Lagrange was later extended by Legendre, Hamilton, and Weierstrass~\citep{paulen2016solution}. It has gained a renewed interest in the mid-twentieth century, as modern calculators offered practical ways of solving some \Gls{acr:ocp}s for nonlinear and time-varying systems that were earlier impracticable~\citep{bryson1975applied}.

The conversion of the calculus of variation problems in \Gls{acr:ocp}s requires the addition of the control variable to the dynamics~\citep{sethi2019optimal}. 

There are numerous methods to analytically and numerically solve these continuous time OCPs, although analytical solution is often impracticable except for very limited state dimensions~\citep{rawlings2017model}. In the early day of optimal control, some analytical solutions were proposed with dynamic programming~\citep{bellman1957dynamic}, and with maximum (or minimum) principle~\citep{pontryagin1962mathematical}. 

In computer science dynamic programming is fundamental to compute optimal solutions, yet it's original form was developed to solve optimal control problems~\citep{lavalle2006planning}. Dynamic programming in optimal control theory is based on a partial differential equation of the performance index named Hamilton-Jacobi-Bellman (HJB) equation, which is solved either analytically for small dimensional state space problems, or numerically~\citep{rawlings2017model}. Dynamic programming can be shown to be equivalent to the principle~\citep{paulen2016solution}. The principle is related to HJB equation in that it provides optimality conditions an optimal trajectory must satisfy~\citep{lavalle2006planning}. HJB offer sufficient conditions for optimality while the principle necessary; yet it is useful to find suitable candidates for optimality~\citep{lavalle2006planning}.

All the numerical approaches discretize infinite-dimensional problems at a certain point~\citep{rawlings2017model}.

A first class of these approaches solves the optimality conditions in continuous time using first-order necessary conditions of optimality from the principle~\citep{bohme2017indirect}. This is done by algebraic manipulation using an expression that is similar to the HJB equation, and results in a boundary-value problem (\Gls{acr:bvp})~\citep{rawlings2017model}. The class is commonly referred to as the indirect methods. The \Gls{acr:bvp} is solved by discretization at the very end~\citep{rawlings2017model} and/or gradient-based resolution~\citep{paulen2016solution}.

On the contrary, modern optimal control often first discretize an the control and state variables in the \Gls{acr:ocp} to a finite dimensional optimization problem (usually \Gls{acr:nlp}), which is then solved with numerical optimization (using gradient-based techniques). This other class of numerical approaches is referred to as direct methods. Some direct methods are single and multiple shooting and collocation methods. We employ direct methods in this chapter.

Modern \Gls{acr:ocp}s are often solved on a finite and receding horizon using an approximation of the true dynamics using \Gls{acr:mpc} techniques. It is a more systematic technique which allow to control a model by re-optimizing the \Gls{acr:ocp} repeatedly~\citep{poe2017process,paulen2016solution}. It takes into account external interferences by re-estimating the model's state (with techniques that we introduced in \fref{cp:est}{Chapter}). \Gls{acr:mpc} is extensively treated in modern optimal control literature~\citep{rawlings2017model,wang2009model,camacho2007model,kwon2006receding,rossiter2004model}.  

%Systems characterized by continuous dynamics are often approximated by multistage systems; if the time step become small enough, these problem might be considered as optimal programming problems for multistage systems. Mathematical programming problems for continuous systems are in fact problems in the calculus of variation~\citep{bryson1975applied}. 
 
\section{Optimization Problems with Dynamics}

\subsection{Continuous systems: unconstrained case}
\label{cp:opt:unconstrained}

Given a state variable $\mathbf{q}$ composed of $m$ states and a control variable $\mathbf{u}$ composed of $n$ controls, the state variable dynamics at a given time instant $t$ can be described by a differential model
\begin{equation}\label{eq:optimization:perfect-model}
    \dot{\mathbf{q}}(t)=f(\mathbf{q}(t),\mathbf{u}(t),t),\,\,\,\mathbf{q}(t_0)=\mathbf{q}_0\,\,\,\text{given},\,\,\,\forall t\in[t_0,T],
\end{equation}
where $t_0\in\mathbb{R}_{\geq 0}$ is a given initial time instant, and $\mathbf{q}_0\in\mathbb{R}^m$ a given initial state guess. The latest can be derived empirically from a previous execution or using some initial sensor data. $f:\mathbb{R}^m\times\mathbb{R}^n\times\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}^m$ maps the current state, control and time to the next state. The notations for $\dot{\mathbf{q}}(t):=d\mathbf{q}(t)/dt$, $\mathbf{q}$, and $\mathbf{u}$ are the same from \fref{cp:model}{Chapter}. The function $f$ is assumed to be continuously differentiable. Physically, \frefeq{eq:optimization:perfect-model} specifies the instantaneous change in state variable of a perfect model with no disturbances.

If the control trajectory $\mathbf{u}(t_0),\mathbf{u}(t_1),\dots,\mathbf{u}(T-\Delta t)$ is known for a given time horizon $t_0\leq t\leq T$, the model in \frefeq{eq:optimization:perfect-model} can be derived to obtain the state trajectory $\mathbf{q}(t_0),\mathbf{q}(t_1),\dots,\mathbf{q}(T)$, where $\Delta t$ is the instantaneous change in time. The last state at the final time instant $T$ is derived from the last control at the time instant $T-\Delta t$. The state trajectory has indeed one item more than the control trajectory.

Optimal control finds a control trajectory which maximizes (or minimizes) a performance index
\begin{equation}
    L=l_f(\mathbf{q}(T),T)+\int_{t_0}^T{l(\mathbf{q}(t),\mathbf{u}(t),t)\,dt},
\end{equation}
where $l,l_f$ are given instantaneous and final cost functions. The instantaneous cost function maps state, controls, and time to a value that quantifies the cost of a given instant $l:\mathbb{R}^m\times\mathbb{R}^n\times\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}$. The final cost function maps the state and time to a value which quantifies the cost of the final instant $l_f:\mathbb{R}^m\times\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}$. The performance index $L\in\mathbb{R}$ is then the sum of all the contribution on the time horizon.

Performance index found in~\citep{bryson1975applied} is also found in literature as cost function in~\citep{simon2006optimal,stengel1994optimal}, objective function in~\citep{rao2019engineering,sethi2019optimal,rawlings2017model}, or performance measure~\citep{kirk2004optimal}.

The control variable $\mathbb{u}$ is usually constrained
\begin{equation}\label{eq:optimization:control-constraint-set}
    \mathbf{u}(t)\in\mathcal{U}(t),\,\,\,\forall t\in[t_0,T],
\end{equation}
where $\,\mathcal{U}(t)\subseteq \mathbb{R}^m$ is the control constraint set. It delimits all the feasible values of the control for the horizon. There can be different control constraint sets for different instants.

The \frefeqM{eq:optimization:perfect-model}{eq:optimization:control-constraint-set} forms unconstrained \Gls{acr:ocp}s. These problems are formalized
\begin{equation}\label{eq:optimization:unconstrained-opt-control-pb}\begin{split}
    \max_{\mathbf{\mathbf{q}(t),\mathbf{u}(t)}}&{l_f(\mathbf{q}(T),T)+\int_{t_0}^T{l(\mathbf{q}(t),\mathbf{u}(t),t)\,dt}},\\
    \text{s.t. }\dot{\mathbf{q}}(t)&=f(\mathbf{q}(t),\mathbf{u}(t),t),\\
    \mathbf{u}(t)&\in\,\mathcal{U}(t),\,\,\,\mathbf{q}(t)\in\,\mathcal{Q}(t),\\
    \mathbf{q}(t_0)&=\mathbf{q}_0\,\,\,\text{given}.
\end{split}\end{equation}

%A simplistic controller which uses \frefeq{eq:optimization:unconstrained-opt-control-pb} is shown in \fref{fig:unconstrained-controller}{Fig.}.
%\begin{figure}[!h]
%    \centering
%    \footnotesize
%    \begin{tikzpicture}[auto, node distance=3.2cm,>=latex']
%        \node [input, name=rinput] (rinput) {};
%        \node [block, right of=rinput] (controller) {~~~$\argmax_{\mathbf{u}(t)}{L}$~~~};
%        \node [right of=controller,node distance=2.8cm] (routput) {\input{figures/opterra-bp.tikz}};
%        \node [right of=routput,node distance=3.6cm] (routput2) {};
%        \draw [->] (rinput) -- node{$\dot{\mathbf{q}}(t),\mathbf{q}_0,t_0,T,\mathcal{U}(t)$} (controller);
%        \draw [->] (controller) -- node{$\mathbf{u}(t)$} (routput);
%        \draw [->] (routput) -- node{path/computations} (routput2);
%    \end{tikzpicture}
%    \caption[Unconstrained state optimal control trajectory controller]{Controller that select the optimal control trajectory from the unconstrained \Gls{acr:ocp} in \frefeq{eq:optimization:unconstrained-opt-control-pb}.}
%    \label{fig:unconstrained-controller}
%\end{figure}
The evolution of the model is used to derive an optimal control trajectory $\mathbf{u}(t)$ from an initial guess of the state $\mathbf{q}_0$ and the horizon. This initial simplistic controller does not represent a realistic scenario. The controller implies that the horizon is known. However, it is often the case that only the initial time step of the horizon $[t_0,T]$ is known. In the model from \fref{cp:model}{Chapter} it is indeed unknown apriori when the aerial robot plan terminates. Moreover the controller does not include any constraint on the state $\mathbf{q}$, although mobile robots are often bounded by strict battery requirements. Lastly, the optimal control generated with such controller is static given the initial state and the horizon. It is unrealistic to assume that the state of the aerial robot travelling the optimal control $\mathbf{u}$ does not change for instants $t_0+\Delta t,t_0+2\Delta t,\dots,T$.

All these initial assumption (known final time step, absence of state constraints, static optimal control law) will be eased in the remaining of the chapter.

\subsection{\color{red}Continuous systems: constrained case}
\label{sec:opt-constrained}

\subsection{\color{orange}Perturbed systems}

\subsection{\color{orange}Multistage systems}


\section{\color{red}Numerical Simulation and Differentiation}

\subsection{\color{red}Euler method}
\label{sec:euler}

\subsection{\color{red}Runge-Kutta methods}
\label{sec:rk4}

\subsection{\color{orange}Algorithmic differentiation}


\section{\color{red}Direct Optimal Control Methods}

\subsection{\color{red}Direct single shooting}

\subsection{\color{red}Direct multiple shooting}

\subsection{\color{orange}Direct collocation}


\section{\color{red}Numerical Optimization}

\subsection{\color{red}Convexity}

\subsection{\color{red}Optimality conditions}

\subsection{\color{orange}First order optimality conditions}

\subsection{\color{orange}Second order optimality conditions}

\subsection{\color{orange}Sequential quadratic programming}

\subsection{\color{orange}Nonlinear interior point methods}


\section{\color{red}Model Predictive Control}



\subsection{Output model predictive control}

In this section, we address the problem of generating the optimal control over a finite time horizon $N$ for an estimated state $\hat{\mathbf{q}}$. The control is the configuration of the path and computations parameters, and it differs from the nominal control (see \fref{sec:nom-cont}{Subsection}). 

To derive the optimal control, we use the estimated state $\hat{\mathbf{q}}$ in \fref{cp:est}{Chapter}, opposed to the perfect state in \fref{cp:model}{Chapter}. The estimated state differs from the perfect state $\mathbf{q}$ due to the uncertainty. The name of output model predictive control in the literature refers to the notion that some available outputs are used to estimate the not fully known state~\citep{rawlings2017model}. For a differential model, such as the periodic model in \frefeq{eq:state-perf} in \fref{sec:periodic-model}{Subsection}, state estimation is done utilizing techniques such as Kalman filtering (see \fref{cp:est}{Chapter}).

Before defining and solving the OCP in the case of uncertainty over the finite horizon, let us re-evaluate the output constraints. The output of the model in \frefeq{eq:state-perf} is the instantaneous energy consumption $y$. We stated earlier that it evolves in $\mathbb{R}$. This is an inexact simplification as physical mobile robots are bounded by strict energy budgets due to battery limitations (see \fref{sec:motivation}{Section}).

Let us hence redefine the original output constraint ($\mathbb{R}$) to include the battery model in \fref{sec:battery-model}{Subsection}. We consider SoC $b$ of the mobile robot's battery with the simplistic difference model in \frefeqM{eq:battery-model-1}{eq:battery-model-2}
\begin{equation}\label{eq:bat}
  \dot{b}(y(t),t)=-k_b\left(V-
  \sqrt{
    V^2-
    4R_ry(t)}
  \right)/(2R_rQ_c),
\end{equation}
where $k_b$ is the battery coefficient determined experimentally,  $V\in\mathbb{R}$ is the internal battery voltage measured in volts, $R_r\in\mathbb{R}$ the resistance measured in ohms, and $Q_c\in\mathbb{R}$ the constant nominal capacity measured in amperes per hour. 

We note that the maximum instantaneous energy consumption can be computed by multiplying the constant nominal capacity, the SoC, and the internal battery voltage. We assume the maximum energy consumption cannot be negative
\begin{equation}
  0\leq y(t)\leq b(y(t),t)Q_cV.
\end{equation}

Therefore, we define a time-varying constraint for the output in \fref{def:const}{Definition}, being the maximum instantaneous energy consumption dependent on the SoC $b$ from \frefeq{eq:bat}, which is dependent on time and the instantaneous energy consumption (at the previous time step).

\begin{highlight}
\begin{defn}[Output constraint]\label{def:const}
The output constraint is the set
\begin{equation*}
  \mathcal{Y}(t):=\{y\mid y\in[0,b(y(t),t)Q_cV]\subseteq{\mathbb{R}_{\geq 0}}\},
\end{equation*}
where $b(y(t),t)Q_cV$ is the maximum instantaneous energy consumption.
\end{defn}
\end{highlight}

We assume the mobile robot carries a battery energy sensor and obtain the initial state of charge $b(y(t_0),t_0)$ accordingly.

The evaluation of the output constraint requires numerical simulation being the battery model in \frefeq{eq:bat} differential, similarly to the energy dynamics of the periodic model in \frefeq{eq:state-perf}. The numerical simulation can be computed using the Euler method in \fref{sec:euler}{Subsectiion}. or the Runge-Kutta method in \fref{sec:rk4}{Subsection}.

The OCP can be stated similarly as in \fref{sec:opt-constrained}{Subsection}, with the constraints: the control constraint in \frefeq{eq:const-set}, the output constraint in \fref{def:const}{Definition}, and the dynamics with the ideal state evolution in \frefeq{eq:state-perf}
\begin{subequations}\begin{align}
   \max_{\mathbf{q}(t),\mathbf{c}_i(t)}&{l_f(\mathbf{q}(T),T)+\int_{t_0}^T{l(\mathbf{q}(t),\mathbf{c}_i(t),t)\,dt}},\\
   \text{s.t. }\dot{\mathbf{q}}&=f(\mathbf{q}(t),\mathbf{c}_i(t),t),\\
   \mathbf{c}_i(t)&\in\mathcal{U}_i,\mathbf{q}(t)\in\mathbb{R}^m,\label{eq:state-cont-const-mpc}\\
   y(t)&\in\mathcal{Y}(t),\label{eq:batt-const-mpc}\\
   q(t_0)&=\mathbf{q}_0\,\,\,\text{given},\text{ and}\\
   b(y(t_0),t_0)&=b_0\,\,\,\text{given}.
\end{align}\end{subequations}

The sizes of the state and control ($m$ and $n$) are defined in \fref{sec:periodic-model}{Subsectoion} and \fref{sec:nom-cont}{Subsection} 

The dynamic evolution is then the periodic model in \frefeq{eq:state-perf} together with the scale transformation from \fref{sec:merging}{Subsection}
\begin{equation}\label{eq:perf-model-in-mpc}
  f(\mathbf{q}(t),\mathbf{c}_i(t),t)=A\mathbf{q}(t)+B\mathrm{diag}(\nu_i)(\mathbf{c}_i(t)-\mathbf{c}_i(t-1)),
\end{equation}
where $\mathbf{c}_i(t-1)$ is the control at the time instant preceding $t$.

The instantaneous cost function is defined with the quadratic expression
\begin{equation}\label{eq:insta-cost-mpc}
  l(\mathbf{q}(t),\mathbf{c}_i(t),t)=\mathbf{q}'(t)Q\mathbf{q}(t)+\mathbf{c}_i'(t)R\mathbf{c}_i(t),
\end{equation}
where $Q\in\mathbb{R}^{m\times m},R\in\mathbb{R}^{n\times n}$ are positive semidefinite matrices.

The final cost function is alike defined with a quadratic expression but with no control
\begin{equation}\label{eq:final-cost-mpc}
  l_f(\mathbf{q}(T),T)=\mathbf{q}'(T)Q_f\mathbf{q}(T),
\end{equation}
where $Q_f\in\mathbb{R}^{m\times m}$ is a positive semidefinite matrix.

The optimization horizon is limited to $N$ measured in seconds, meaning the controller will select the optimal control trajectory $\mathbf{c}_i^*(t)$ over $[t_0,T]$, with $T=t_0+N$.

It selects the optimal control trajectory as follows.
At each time step the controller refines the control trajectory which respects the constraints: the dynamics in \frefeq{eq:perf-model-in-mpc}, and the control constraint in \frefeq{eq:state-cont-const-mpc}, and maximizes the instantaneous cost function $l$ in \frefeq{eq:insta-cost-mpc}. The controller refines similarly the state trajectory which respects the dynamics, the state constraint in \frefeq{eq:state-cont-const-mpc}, and the battery constraint in \frefeq{eq:batt-const-mpc}. It maximizes both the instantaneous cost function $l$, and the final cost function $l_f$ in \frefeq{eq:final-cost-mpc}.

The dynamics constraint satisfaction requires to evolve the perfect model $f$ in \frefeq{eq:perf-model-in-mpc} over horizon $[t_0,t_0+N]$ beginning from the last estimated state $\mathbf{q}_0=\hat{\mathbf{q}}(t_0)$ at time instant $t_0$. The battery constraint is likewise evolved beginning from the last battery measurement $b_0$ obtained from the battery energy sensor also at time instant $t_0$.

\subsection{\color{red}Optimal control generation with model predictive control}
\label{sec:opt-cont-gener}

\section{\color{red}Results}


\section{\color{red}Summary}

