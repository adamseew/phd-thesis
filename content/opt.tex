
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                              %
% Optimal Control Generation   %
%                              %
\chapter{Optimal Control Generation}
\label{cp:opt}

%\begin{highlight}
%    \begin{st}
%        Initial version up to \fref{cp:opt:unconstrained}{Subsection}. From \fref{cp:opt:constrained}{Subsection} dummy text.
%    \end{st} 
%\end{highlight}

\lettrine{T}{his} chapter provides essential theoretical background on optimal control theory necessary to derive an optimal configuration of the path and computations of the flying \Gls{acr:uav}. It solves the problem posed in~\fref{cp:intro:pb}{Section} and illustrate an algorithm that generates the optimal configuration dynamically. The algorithm relies on a modern optimal control technique known as model predictive control (\Gls{acr:mpc}), where the optimal control trajectory is evaluated on a receding horizon for each optimization step~\citep{rawlings2017model}. 

Optimal control deals with finding optimal ways to control a dynamic system~\citep{sethi2019optimal}. It determines the control signal--the evolution in time of the decision variables--such that the model satisfies the dynamics and simultaneously optimizes a performance index~\citep{kirk2004optimal}.

Many optimization problems originating in fields such as robotics, economics, and aeronautics can be formulated as optimal control problems~(\Gls{acr:ocp}s)~\citep{von1992direct}. Optimization is often called mathematical programming~\citep{nocedal2006numerical} a term that means finding ways to solve the optimization problem. One can often find programming in this context in terms such as linear program (\Gls{acr:lp}), quadratic program (\Gls{acr:qp}), and nonlinear program (\Gls{acr:nlp}). \Gls{acr:nlp}s is the class of optimization problems that we use to derive the optimal configuration. \Gls{acr:ocp}s can be seen as optimization problems with the added difficulty of continuous dynamics. The latter is to be integrated over the optimization horizon using numerical simulation. In the algorithm, we formulate the dynamic planning problem as an \Gls{acr:ocp} that we solve with a numerical method: we transform the \Gls{acr:ocp} in an \Gls{acr:nlp} using numerical simulation and solve the \Gls{acr:nlp} using numerical optimization, as proposed in~\citep{rawlings2017model}. The process is illustrated in \fref{fig:opt:process-summary}{Fig.}.

\begin{figure}[!h]
    \centering
    \footnotesize
    \begin{tikzpicture}[auto, node distance=3.2cm,>=latex']
        \node [input, name=rinput] (rinput) {};
        \node [right of=rinput] (uav) {\input{figures/opterra-bp.tikz}};
        \node [block,right of=uav,node distance=2.2cm] (theblock) {\Gls{acr:ocp}};
        \node [block,right of=theblock,node distance=2.2cm] (theblock2) {\Gls{acr:nlp}};
        \draw [->] (uav) -- node{} (theblock);
        \draw [->] (theblock) -- node{} (theblock2);
        \draw [->] (theblock2) -- ++(0,-1cm) -| node[pos=0.23] {path/computations} 
        node [near end] {} (uav);
        \draw [decorate,decoration={brace,amplitude=5pt,raise=26pt},yshift=0pt] (theblock.north west) -- (theblock2.north east) node [black,midway,yshift=10ex]{numerical simulation};

        \draw [decorate,decoration={brace,amplitude=5pt,raise=4pt},yshift=0pt] (theblock2.north west) -- (theblock2.north east) node [black,midway,yshift=3ex]{numerical optimization};
        \draw [decorate,decoration={brace,amplitude=5pt,raise=4pt},yshift=0pt] (uav.north west) -- (uav.north west -| theblock.north east) node [black,midway,sloped,yshift=3ex]{dynamic planing problem};
    \end{tikzpicture}
    \caption[Summary of the optimal control approach]{Summary of the optimal control approach. The problem is formulated as an \Gls{acr:ocp}, into finite-dimensional discrete \Gls{acr:nlp} using numerical simulation. \Gls{acr:nlp} is solved using numerical optimization and the optimal configuration for a given time horizon is returned to the UAV. The following horizon is evaluated again in a technique known as \Gls{acr:mpc}.}
    \label{fig:opt:process-summary}
\end{figure}

A typical performance measure for an \Gls{acr:ocp} is built such that the system: reaches a target set $\mathcal{Q}_f$ in minor time, reaches a given final state $\mathbf{q}_f$ with minimum deviation, maintains the state evolution as close as possible to a given desired evolution, or reaches the target set with the minimum control expenditure effort~\citep{kirk2004optimal}. In energy planning, it is desired to focus on the latter performance measure. 

The outline of the chapter is as follows. After a brief history of optimal control, we introduce formally the \Gls{acr:ocp} subject to continuous dynamics. We then illustrate numerical simulation approaches to convert the infinite-dimensional continuous dynamics into finite-dimensional discrete dynamics. We formulate later in the chapter the dynamic planning problem for the optimal configuration of the path and computations with proper constraints. Finally, we illustrate \Gls{acr:mpc} to solve \Gls{acr:ocp} on a receding horizon. We propose an algorithm to solve such \Gls{acr:ocp} using a numerical method on the horizon along with the analysis of its practical feasibility.

The chapter builds on the rest of the work as follows. In the \Gls{acr:ocp} formulation, we use the estimated state from \fref{cp:est}{Chapter} of a perfect model in \fref{cp:model}{Chapter} to solve the planning problem in \fref{cp:intro:pb}{Section} and guide the UAV with the obtained optimal configuration from this chapter with the technique in \fref{cp:gd}{Chapter}.

\section{A Brief History of Optimal Control}

Optimal control originates from the calculus of variations~\citep{sethi2019optimal}, based on the work of Euler and Lagrange. Calculus of variations solves the problem of determining the arguments of an integral, such that its value is maximum (or minimum). The equivalent problem in calculus is to determine the argument of a function where the function is maximum (or minimum). The work by Euler and Lagrange was later extended by Legendre, Hamilton, and Weierstrass~\citep{paulen2016solution}. It has gained a renewed interest in the mid-twentieth century, as modern calculators offered practical ways of solving some \Gls{acr:ocp}s for nonlinear and time-varying systems that were earlier impracticable~\citep{bryson1975applied}.

The conversion of the calculus of variation problems in \Gls{acr:ocp}s requires the addition of the control variable to the dynamics~\citep{sethi2019optimal}. 

There are numerous methods to analytically and numerically solve these continuous time OCPs, although analytical solution is often impracticable except for very limited state dimensions~\citep{rawlings2017model}. In the early day of optimal control, some analytical solutions were proposed with dynamic programming~\citep{bellman1957dynamic}, and with maximum (or minimum) principle~\citep{pontryagin1962mathematical}. 

In computer science dynamic programming is fundamental to compute optimal solutions, yet it's original form was developed to solve optimal control problems~\citep{lavalle2006planning}. Dynamic programming in optimal control theory is based on a partial differential equation of the performance index named Hamilton-Jacobi-Bellman (HJB) equation, which is solved either analytically for small dimensional state space problems, or numerically~\citep{rawlings2017model}. Dynamic programming can be shown to be equivalent to the principle~\citep{paulen2016solution}. The principle is related to HJB equation in that it provides optimality conditions an optimal trajectory must satisfy~\citep{lavalle2006planning}. HJB offer sufficient conditions for optimality while the principle necessary; yet it is useful to find suitable candidates for optimality~\citep{lavalle2006planning}.

All the numerical approaches discretize infinite-dimensional problems at a certain point~\citep{rawlings2017model}.

A first class of these approaches solves the optimality conditions in continuous time using first-order necessary conditions of optimality from the principle~\citep{bohme2017indirect}. This is done by algebraic manipulation using an expression that is similar to the HJB equation, and results in a boundary-value problem (\Gls{acr:bvp})~\citep{rawlings2017model}. The class is commonly referred to as the indirect methods. The \Gls{acr:bvp} is solved by discretization at the very end~\citep{rawlings2017model} and/or gradient-based resolution~\citep{paulen2016solution}.

On the contrary, modern optimal control often first discretize an the control and state variables in the \Gls{acr:ocp} to a finite dimensional optimization problem (usually \Gls{acr:nlp}), which is then solved with numerical optimization (using gradient-based techniques). This other class of numerical approaches is referred to as direct methods. Some direct methods are single and multiple shooting and collocation methods. We employ direct methods in this chapter.

Modern \Gls{acr:ocp}s are often solved on a finite and receding horizon using an approximation of the true dynamics using \Gls{acr:mpc} techniques. It is a more systematic technique which allow to control a model by re-optimizing the \Gls{acr:ocp} repeatedly~\citep{poe2017process,paulen2016solution}. It takes into account external interferences by re-estimating the model's state (with techniques that we introduced in \fref{cp:est}{Chapter}). \Gls{acr:mpc} is extensively treated in modern optimal control literature~\citep{rawlings2017model,wang2009model,camacho2007model,kwon2006receding,rossiter2004model}.  

%Systems characterized by continuous dynamics are often approximated by multistage systems; if the time step become small enough, these problem might be considered as optimal programming problems for multistage systems. Mathematical programming problems for continuous systems are in fact problems in the calculus of variation~\citep{bryson1975applied}. 
 
\section{Optimization Problems with Dynamics}

\subsection{Continuous systems: unconstrained case}
\label{cp:opt:unconstrained}

Given a state variable $\mathbf{q}$ composed of $m$ states and a control variable $\mathbf{u}$ composed of $n$ controls, the state variable dynamics at a given time instant $t$ can be described by a differential model
\begin{equation}\label{eq:optimization:perfect-model}
    \dot{\mathbf{q}}(t)=f(\mathbf{q}(t),\mathbf{u}(t),t),\,\,\,\mathbf{q}(t_0)=\mathbf{q}_0\,\,\,\text{given},\,\,\,\forall t\in[t_0,T],
\end{equation}
where $t_0\in\mathbb{R}_{\geq 0}$ is a given initial time instant, and $\mathbf{q}_0\in\mathbb{R}^m$ a given initial state guess. The latest can be derived empirically from a previous execution or using some initial sensor data. $f:\mathbb{R}^m\times\mathbb{R}^n\times\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}^m$ maps the current state, control and time to the next state. The notations for $\dot{\mathbf{q}}(t):=d\mathbf{q}(t)/dt$, $\mathbf{q}$, and $\mathbf{u}$ are the same from \fref{cp:model}{Chapter}. The function $f$ is assumed to be continuously differentiable. Physically, \frefeq{eq:optimization:perfect-model} specifies the instantaneous change in state variable of a perfect model with no disturbances.

If the control trajectory $\mathbf{u}(t_0),\mathbf{u}(t_1),\dots,\mathbf{u}(T-\Delta t)$ is known for a given time horizon $t_0\leq t\leq T$, the model in \frefeq{eq:optimization:perfect-model} can be derived to obtain the state trajectory $\mathbf{q}(t_0),\mathbf{q}(t_1),\dots,\mathbf{q}(T)$, where $\Delta t$ is the instantaneous change in time. The last state at the final time instant $T$ is derived from the last control at the time instant $T-\Delta t$. The state trajectory has indeed one item more than the control trajectory.

Optimal control finds a control trajectory which maximizes (or minimizes) a performance index
\begin{equation}
    L=l_f(\mathbf{q}(T),T)+\int_{t_0}^{T}{l(\mathbf{q}(t),\mathbf{u}(t),t)\,dt},
\end{equation}
where $l,l_f$ are given instantaneous and final cost functions. The instantaneous cost function maps state, controls, and time to a value that quantifies the cost of a given instant $l:\mathbb{R}^m\times\mathbb{R}^n\times\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}$. The final cost function maps the state and time to a value which quantifies the cost of the final instant $l_f:\mathbb{R}^m\times\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}$. The performance index $L\in\mathbb{R}$ is then the sum of all the contribution on the time horizon.

Performance index found in~\citep{bryson1975applied} is also found in literature as cost function in~\citep{simon2006optimal,stengel1994optimal}, objective function in~\citep{rao2019engineering,sethi2019optimal,rawlings2017model}, or performance measure~\citep{kirk2004optimal}.

The control variable $\mathbb{u}$ is usually constrained
\begin{equation}\label{eq:optimization:control-constraint-set}
    \mathbf{u}(t)\in\mathcal{U}(t),\,\,\,\forall t\in[t_0,T],
\end{equation}
where $\,\mathcal{U}(t)\subseteq \mathbb{R}^m$ is the control constraint set. It delimits all the feasible values of the control for the horizon. There can be different control constraint sets for different instants.

The \frefeqM{eq:optimization:perfect-model}{eq:optimization:control-constraint-set} forms unconstrained \Gls{acr:ocp}s. These problems are formalized
\begin{equation}\label{eq:optimization:unconstrained-opt-control-pb}\begin{split}
    &\max_{\mathbf{u(t)\in\,\mathcal{U}(t)}}{l_f(\mathbf{q}(T),T)+\int_{t_0}^{T}{l(\mathbf{q}(t),\mathbf{u}(t),t)\,dt}},\\
    &\text{s.t.}\,\,\,\dot{\mathbf{q}}(t)=f(\mathbf{q}(t),\mathbf{u}(t),t)\\
    &\mathbf{q}(t_0)=\mathbf{q}_0\,\,\,\text{given}.
\end{split}\end{equation}

%A simplistic controller which uses \frefeq{eq:optimization:unconstrained-opt-control-pb} is shown in \fref{fig:unconstrained-controller}{Fig.}.
%\begin{figure}[!h]
%    \centering
%    \footnotesize
%    \begin{tikzpicture}[auto, node distance=3.2cm,>=latex']
%        \node [input, name=rinput] (rinput) {};
%        \node [block, right of=rinput] (controller) {~~~$\argmax_{\mathbf{u}(t)}{L}$~~~};
%        \node [right of=controller,node distance=2.8cm] (routput) {\input{figures/opterra-bp.tikz}};
%        \node [right of=routput,node distance=3.6cm] (routput2) {};
%        \draw [->] (rinput) -- node{$\dot{\mathbf{q}}(t),\mathbf{q}_0,t_0,T,\mathcal{U}(t)$} (controller);
%        \draw [->] (controller) -- node{$\mathbf{u}(t)$} (routput);
%        \draw [->] (routput) -- node{path/computations} (routput2);
%    \end{tikzpicture}
%    \caption[Unconstrained state optimal control trajectory controller]{Controller that select the optimal control trajectory from the unconstrained \Gls{acr:ocp} in \frefeq{eq:optimization:unconstrained-opt-control-pb}.}
%    \label{fig:unconstrained-controller}
%\end{figure}
The evolution of the model is used to derive an optimal control trajectory $\mathbf{u}(t)$ from an initial guess of the state $\mathbf{q}_0$ and the horizon. This initial simplistic controller does not represent a realistic scenario. The controller implies that the horizon is known. However, it is often the case that only the initial time step of the horizon $[t_0,T]$ is known. In the model from \fref{cp:model}{Chapter} it is indeed unknown apriori when the UAV plan terminates. Moreover the controller does not include any constraint on the state $\mathbf{q}$, although UAVs are often bounded by strict battery requirements. Lastly, the optimal control generated with such controller is static given the initial state and the horizon. It is unrealistic to assume that the state of the UAV travelling the optimal control $\mathbf{u}$ does not change for instants $t_0+\Delta t,t_0+2\Delta t,\dots,T$.

All these initial assumption (known final time step, absence of state constraints, static optimal control law) will be eased in the remaining of the chapter.

\subsection{\color{red}Continuous systems: constrained case}
\label{cp:opt:constrained}

\subsection{\color{red}Perturbed systems}

\subsection{\color{red}Multistage systems}


\section{\color{red}Numerical Simulation and Differentiation}

\subsection{\color{red}Euler method}
\label{sec:euler}

\subsection{\color{red}Runge-Kutta methods}
\label{sec:rk4}

\subsection{\color{red}Algorithmic differentiation}


\section{\color{red}Direct Optimal Control Methods}

\subsection{\color{red}Direct single shooting}

\subsection{\color{red}Direct multiple shooting}

\subsection{\color{red}Direct collocation}


\section{\color{red}Numerical Optimization}

\subsection{\color{red}Convexity}

\subsection{\color{red}Optimality conditions}

\subsection{\color{red}First order optimality conditions}

\subsection{\color{red}Second order optimality conditions}

\subsection{\color{red}Sequential quadratic programming}

\subsection{\color{red}Nonlinear interior point methods}


\section{\color{red}Model Predictive Control}

\subsection{\color{red}Output model predictive control}

\subsection{\color{red}Optimal control generation with model predictive control}


\section{\color{red}Results}


\section{\color{red}Summary}

