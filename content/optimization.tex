
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                              %
% Optimal Control Generation   %
%                              %
\chapter{Optimal Control Generation}
\label{cp:optimization}

\lettrine{T}{his} chapter provides essential theoretical background on optimal control theory and derives an optimal configuration of the path and computations of the flying \Gls{acr:uav}. The outline of the chapter is as follows. First, the optimization problems for dynamic systems, such as the energy model from \fref{cp:model}{Chapter}, are introduced. A generalized \Gls{acr:nlp} for the optimal configuration of the path and computations under battery constraints is provided later in the chapter. Finally, some optimal controls algorithm to solve the \Gls{acr:nlp} are proposed along with the analysis of their practical feasibility. 

Optimal control deals with finding optimal ways to control a dynamic system~\citep{sethi2019optimal}. It determines the control signal, the evolution in time of the decision variables, such that the model satisfies the dynamics and simultaneously optimize a performance measure~\citep{kirk2004optimal}. 


\section{Optimal Control: Unconstrained Case}

Given a state variable $\mathbf{q}$ composed of $m$ states and a control variable $\mathbf{u}$ composed of $n$ controls, the state variable dynamics at a given time instant $t$ can be described by a differential model
\begin{equation}\label{eq:optimization:perfect-model}
    \dot{\mathbf{q}}(t)=f(\mathbf{q}(t),\mathbf{u}(t),t),\,\,\,\mathbf{q}(t_0)=\mathbf{q}_0\,\,\,\text{given},\,\,\,\forall t\in[t_0,T],
\end{equation}
where $t_0\in\mathbb{R}_{\geq 0}$ is a given initial time instant, and $\mathbf{q}_0\in\mathbb{R}^m$ a given initial state guess. The latest can be derived empirically from a previous execution or using some initial sensor data. $f:\mathbb{R}^m\times\mathbb{R}^n\times\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}^m$ maps the current state, control and time to the next state. The notations for $\dot{\mathbf{q}}(t):=d\mathbf{q}(t)/dt$, $\mathbf{q}$, and $\mathbf{u}$ are the same from \fref{cp:model}{Chapter}. The function $f$ is assumed to be continuously differentiable. Physically, \frefeq{eq:optimization:perfect-model} specifies the instantaneous change in state variable of a perfect model with no disturbances.

If the control trajectory $\mathbf{u}(t_0),\mathbf{u}(t_1),\dots,\mathbf{u}(T-\Delta t)$ is known for a given time horizon $t_0\leq t\leq T$, the model in \frefeq{eq:optimization:perfect-model} can be derived to obtain the state trajectory $\mathbf{q}(t_0),\mathbf{q}(t_1),\dots,\mathbf{q}(T)$, where $\Delta t$ is the instantaneous change in time. The last state at the final time instant $T$ is derived from the last control at the time instant $T-\Delta t$. The state trajectory has indeed one item more than the control trajectory.

Optimal control finds a control trajectory which maximizes (or minimizes) a performance index
\begin{equation}
    L=l_f(\mathbf{q}(T),T)+\int_{t_0}^{T}{l(\mathbf{q}(t),\mathbf{u}(t),t)\,dt},
\end{equation}
where $l,l_f$ are given instantaneous and final cost functions. The instantaneous cost function maps state, controls, and time to a value that quantifies the cost of a given instant $l:\mathbb{R}^m\times\mathbb{R}^n\times\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}$. The final cost function maps the state and time to a value which quantifies the cost of the final instant $l_f:\mathbb{R}^m\times\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}$. The performance index $L\in\mathbb{R}$ is then the sum of all the contribution on the time horizon.

Performance index found in~\citep{bryson1975applied} is also found in literature as cost function in~\citep{simon2006optimal,stengel1994optimal}, objective function in~\citep{rao2019engineering,sethi2019optimal,rawlings2017model}, or performance measure~\citep{kirk2004optimal}.

The control variable $\mathbb{u}$ is usually constrained
\begin{equation}\label{eq:optimization:control-constraint-set}
    \mathbf{u}(t)\in\mathcal{U}(t),\,\,\,\forall t\in[t_0,T],
\end{equation}
where $\,\mathcal{U}(t)\subseteq \mathbb{R}^m$ is the control constraint set. It delimits all the feasible values of the control for the horizon. There can be different control constraint sets for different instants.

The \frefeqM{eq:optimization:perfect-model}{eq:optimization:control-constraint-set} forms unconstrained optimal control problems. These problems are formalized
\begin{equation}\label{eq:optimization:unconstrained-opt-control-pb}\begin{split}
    &\max_{\mathbf{u(t)\in\,\mathcal{U}(t)}}{l_f(\mathbf{q}(T),T)+\int_{t_0}^{T}{l(\mathbf{q}(t),\mathbf{u}(t),t)\,dt}},\\
    &\text{s.t.}\,\,\,\dot{\mathbf{q}}(t)=f(\mathbf{q}(t),\mathbf{u}(t),t)\\
    &\mathbf{q}(t_0)=\mathbf{q}_0\,\,\,\text{given}.
\end{split}\end{equation}

A simplistic controller which uses \frefeq{eq:optimization:unconstrained-opt-control-pb} is shown in \fref{fig:unconstrained-controller}{Fig.}.
\begin{figure}[!h]
    \centering
    \footnotesize
    \begin{tikzpicture}[auto, node distance=3.2cm,>=latex']
        \node [input, name=rinput] (rinput) {};
        \node [block, right of=rinput] (controller) {~~~$\argmax_{\mathbf{u}(t)}{L}$~~~};
        \node [right of=controller,node distance=2.8cm] (routput) {\input{figures/opterra-bp.tikz}};
        \node [right of=routput,node distance=3.6cm] (routput2) {};
        \draw [->] (rinput) -- node{$\dot{\mathbf{q}}(t),\mathbf{q}_0,t_0,T,\mathcal{U}(t)$} (controller);
        \draw [->] (controller) -- node{$\mathbf{u}(t)$} (routput);
        \draw [->] (routput) -- node{path/computations} (routput2);
    \end{tikzpicture}
    \caption[Unconstrained state optimal control trajectory controller]{Controller that select the optimal control trajectory from the unconstrained optimal control problem in \frefeq{eq:optimization:unconstrained-opt-control-pb}.}
    \label{fig:unconstrained-controller}
\end{figure}
The evolution of the model is used to derive an optimal control trajectory $\mathbf{u}(t)$ from an initial guess of the state $\mathbf{q}_0$ and the horizon. This initial simplistic controller does not represent a realistic scenario. The controller implies that the horizon is known. However, it is often the case that only the initial time step of the horizon $[t_0,T]$ is known. In the model from \fref{cp:model}{Chapter} it is indeed unknown apriori when the UAV plan terminates. Moreover the controller does not include any constraint on the state $\mathbf{q}$, although UAVs are often bounded by strict battery requirements. Lastly, the optimal control generated with such controller is static given the initial state and the horizon. It is unrealistic to assume that the state of the UAV travelling the optimal control $\mathbf{u}$ does not change for instants $t_0+\Delta t,t_0+2\Delta t,\dots,T$.

All these initial assumption (known final time step, absence of state constraints, static optimal control law) will be eased in the remaining of the chapter.

Given the set of all feasible controls $\,\mathcal{U}(t)$ for each time step in the horizon, the optimal control trajectory $\mathbf{u}(t)$ for the unconstrained optimal control problem in \frefeq{eq:optimization:unconstrained-opt-control-pb} can be selected with a variety of approaches. 

